# 训练模型

在之前的描述中，我们通常把机器学习模型和训练算法当作黑箱子来处理。如果你实践过前几章的一些示例，你惊奇的发现你可以优化回归系统，改进数字图像的分类器，你甚至可以零基础搭建一个垃圾邮件的分类器，但是你却对它们内部的工作流程一无所知。事实上，许多场合你都不需要知道这些黑箱子的内部有什么，干了什么。

然而，如果你对其内部的工作流程有一定了解的话，当面对一个机器学习任务时候，这些理论可以帮助你快速的找到恰当的机器学习模型，合适的训练算法，以及一个好的假设集。同时，了解黑箱子内部的构成，有助于你更好地调试参数以及更有效的误差分析。本章讨论的大部分话题对于机器学习模型的理解，构建，以及神经网络（详细参考本书的第二部分）的训练都是非常重要的。

首先我们将以一个简单的线性回归模型为例，讨论两种不同的训练方法来得到模型的最优解：

- 直接使用封闭方程进行求根运算，得到模型在当前训练集上的最优参数（即在训练集上使损失函数达到最小值的模型参数）
- 使用迭代优化方法：梯度下降（GD），在训练集上，它可以逐渐调整模型参数以获得最小的损失函数，最终，参数会收敛到和第一种方法相同的的值。同时，我们也会介绍一些梯度下降的变体形式：批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD），在第二部分的神经网络部分，我们会多次使用它们。

接下来，我们将研究一个更复杂的模型：多项式回归，它可以拟合非线性数据集，由于它比线性模型拥有更多的参数，于是它更容易出现模型的过拟合。因此，我们将介绍如何通过学习曲线去判断模型是否出现了过拟合，并介绍几种正则化方法以减少模型出现过拟合的风险。

最后，我们将介绍两个常用于分类的模型：Logistic回归和Softmax回归

> 提示
>
> 在本章中包含许多数学公式，以及一些线性代数和微积分基本概念。为了理解这些公式，你需要知道什么是向量，什么是矩阵，以及它们直接是如何转化的，以及什么是点积，什么是矩阵的逆，什么是偏导数。如果你对这些不是很熟悉的话，你可以阅读本书提供的 Jupyter 在线笔记，它包括了线性代数和微积分的入门指导。对于那些不喜欢数学的人，你也应该快速简单的浏览这些公式。希望它足以帮助你理解大多数的概念。

## 线性回归

在第一章，我们介绍了一个简单的生活满意度回归模型:

[![life_satisfaction = \theta_{0} + \theta_{1} * GDP_per_capita](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2b4fc5fcdceb2e12c666415e9ebb793a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2b4fc5fcdceb2e12c666415e9ebb793a.gif)

这个模型仅仅是输入量`GDP_per_capita`的线性函数，[![\theta _{0}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和 [![\theta _{1}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-92c9134527161fd7453fe848b821d8c7.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-92c9134527161fd7453fe848b821d8c7.gif) 是这个模型的参数，线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。如公式 4-1：

公式 4-1：线性回归预测模型

[![\hat{y} = \theta _{0} + \theta _{1}x _{1}+\theta _{2}x _{2}+\dots+\theta _{n}x _{n}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f99876b625a13a0aad9631f61d934a61.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f99876b625a13a0aad9631f61d934a61.gif)

- [![\hat{y}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-5d28a7ba1a44a73b8c2ed21321697c59.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-5d28a7ba1a44a73b8c2ed21321697c59.gif) 表示预测结果
- [![n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 表示特征的个数
- [![x_{i}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-05e42209d67fe1eb15a055e9d3b3770e.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-05e42209d67fe1eb15a055e9d3b3770e.gif) 表示第`i`个特征的值
- [![\theta_{j}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-4c5645510c6b63f6bd0c770fceea4ac1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-4c5645510c6b63f6bd0c770fceea4ac1.gif) 表示第`j`个参数（包括偏置项 [![\theta _{0}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和特征权重值 [![\theta _{1},\theta _{2},\dots,\theta _{n}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-54552e28fe2d86eb74de3db6abb5aab8.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-54552e28fe2d86eb74de3db6abb5aab8.gif)）

上述公式可以写成更为简洁的向量形式，如公式 4-2：

公式 4-2：线性回归预测模型（向量形式）

[![\hat{y} = h _{\theta} (\mathbf{x})= \theta^T  \cdot \mathbf{x}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-5da22015388cdeacf9c75c3511592953.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-5da22015388cdeacf9c75c3511592953.gif)

- [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 表示模型的参数向量包括偏置项 [![\theta _{0}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3d7e996adfe310516ca31c796df1ce8c.gif) 和特征权重值 [![\theta _{1}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-92c9134527161fd7453fe848b821d8c7.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-92c9134527161fd7453fe848b821d8c7.gif) 到 [![\theta _{n}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f5c4858c5101df83fa23d1f57981c12b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f5c4858c5101df83fa23d1f57981c12b.gif)
- [![\theta^T](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-616944242afb697770bd2354c57e4773.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-616944242afb697770bd2354c57e4773.gif) 表示向量[![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)的转置（行向量变为了列向量）
- [![\mathbf{x}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-70e59a996bd69a0c21878b4093375e92.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-70e59a996bd69a0c21878b4093375e92.gif) 为每个样本中特征值的向量形式，包括 [![x _{1}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f1dd81ad21180e074e52e8cccdcbb172.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f1dd81ad21180e074e52e8cccdcbb172.gif) 到 [![x_{n}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-67b68721103b5a16194f4b3e3ec222db.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-67b68721103b5a16194f4b3e3ec222db.gif)，而且 [![x_0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3e0d691f3a530e6c7e079636f20c111b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3e0d691f3a530e6c7e079636f20c111b.gif) 恒为 1
- [![\theta^T  \cdot \mathbf{x}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-dc285d84f74bea2336104ee5eafff150.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-dc285d84f74bea2336104ee5eafff150.gif) 表示 [![\theta^T](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-616944242afb697770bd2354c57e4773.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-616944242afb697770bd2354c57e4773.gif) 和[![  \mathbf{x}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7186298c04fa42047e7992afaf52dfe0.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7186298c04fa42047e7992afaf52dfe0.gif) 的点积
- [![h_{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 表示参数为 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 的假设函数

怎么样去训练一个线性回归模型呢？好吧，回想一下，训练一个模型指的是设置模型的参数使得这个模型在训练集的表现较好。为此，我们首先需要找到一个衡量模型好坏的评定方法。在第二章，我们介绍到在回归模型上，最常见的评定标准是均方根误差（RMSE，详见公式 2-1）。因此，为了训练一个线性回归模型，你需要找到一个 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 值，它使得均方根误差（标准误差）达到最小值。实践过程中，最小化均方误差比最小化均方根误差更加的简单，这两个过程会得到相同的 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)，因为函数在最小值时候的自变量，同样能使函数的方根运算得到最小值。

在训练集 [![\mathbf{X}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-ca340abf4b48dc6d816137fbadf58b53.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-ca340abf4b48dc6d816137fbadf58b53.gif) 上使用公式 4-3 来计算线性回归假设 [![h_{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 的均方差（[![MSE](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a9fc1a03386ae38b64e06c8172994963.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a9fc1a03386ae38b64e06c8172994963.gif)）。

公式 4-3：线性回归模型的 MSE 损失函数

[![MSE (\mathbf{X},h_{\theta}) = \frac{1}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e42dee8953b9b2be4a3ed6f8c09e5314.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e42dee8953b9b2be4a3ed6f8c09e5314.gif)

公式中符号的含义大多数都在第二章（详见“符号”）进行了说明，不同的是：为了突出模型的参数向量 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)，使用 [![h_{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-b7ae93cd51ba1d38fe4ebac15eab3aa1.gif) 来代替 [![h](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2510c39011c5be704182423e3a695e91.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2510c39011c5be704182423e3a695e91.gif)。以后的使用中为了公式的简洁，使用 [![MSE(\theta)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e6e3116680e8cef8739f29e51e9ae4dc.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e6e3116680e8cef8739f29e51e9ae4dc.gif) 来代替 [![MSE(\mathbf{X},h _{\theta})](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-8fdec996997ab7fb44bf97399fda93c7.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-8fdec996997ab7fb44bf97399fda93c7.gif)。

### 正态方程

为了找到最小化损失函数的 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 值，可以采用公式解，换句话说，就是可以通过解正态方程直接得到最后的结果。

公式 4-4：正态方程

[![\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-43bfb04cdbbd85ad21489e8e2dc853ed.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-43bfb04cdbbd85ad21489e8e2dc853ed.gif)

- [![\hat{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif) 指最小化损失 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 的值
- [![\mathbf{y}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-971f2023c1f5f54b8bd389bb06fa6d86.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-971f2023c1f5f54b8bd389bb06fa6d86.gif) 是一个向量，其包含了 [![y^{(1)}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a2c0f1b552d410257a2cc027b24757a9.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a2c0f1b552d410257a2cc027b24757a9.gif) 到 [![y^{(m)}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a4bd3c895fe9f194e30d5ce53dbb5fee.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a4bd3c895fe9f194e30d5ce53dbb5fee.gif) 的值

让我们生成一些近似线性的数据（如图 4-1）来测试一下这个方程。

```
import numpy as np 
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
```

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-1.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-1.PNG)

图 4-1：随机线性数据集

现在让我们使用正态方程来计算 [![\hat{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif)，我们将使用 Numpy 的线性代数模块（`np.linalg`）中的`inv()`函数来计算矩阵的逆，以及`dot()`方法来计算矩阵的乘法。

```
X_b = np.c_[np.ones((100, 1)), X] 
theta_best = np.linalg.inv(X_b.T.dot(X_B)).dot(X_b.T).dot(y)
```

我们生产数据的函数实际上是 [![y = 4 + 3x_0 + 高斯噪声](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-751d6173162c5bb7b6294ca57e03d5b1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-751d6173162c5bb7b6294ca57e03d5b1.gif)。让我们看一下最后的计算结果。

```
>>> theta_best
array([[4.21509616],[2.77011339]])
```

我们希望最后得到的参数为 [![\theta_0=4,\theta_1=3](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e62a4db03666029d1dc53713c5632ac6.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e62a4db03666029d1dc53713c5632ac6.gif) 而不是 [![\theta_0=3.865,\theta_1=3.139](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-4443b7e9e72b482475432b88c0e4fdd8.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-4443b7e9e72b482475432b88c0e4fdd8.gif) （译者注：我认为应该是 [![\theta_0=4.2150,\theta_1=2.7701](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-74747d7f311ae50d3c361e82606617d1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-74747d7f311ae50d3c361e82606617d1.gif)）。这已经足够了，由于存在噪声，参数不可能达到到原始函数的值。

现在我们能够使用 [![\hat{\theta}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-0678caa04da34220a4e8dc041488b618.gif) 来进行预测：

```
>>> X_new = np.array([[0],[2]])
>>> X_new_b = np.c_[np.ones((2, 1)), X_new]
>>> y_predict = X_new_b.dot(theta.best)
>>> y_predict
array([[4.21509616],[9.75532293]])
```

画出这个模型的图像，如图 4-2

```
plt.plot(X_new,y_predict,"r-")
plt.plot(X,y,"b.")
plt.axis([0,2,0,15])
plt.show()
```

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-2.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-2.PNG)

图4-2：线性回归预测

使用下面的 Scikit-Learn 代码可以达到相同的效果：

```
>>> form sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X,y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([4.21509616]),array([2.77011339]))
>>> lin_reg.predict(X_new)
array([[4.21509616],[9.75532293]])
```

### 计算复杂度

正态方程需要计算矩阵 [![{\mathbf{X}}^T\cdot\mathbf{X}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e5949d4f83eb4e13761f2b76ab62386e.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e5949d4f83eb4e13761f2b76ab62386e.gif) 的逆，它是一个 [![n * n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-6679fc33e499a90a99b97201f4d00ed5.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-6679fc33e499a90a99b97201f4d00ed5.gif) 的矩阵（[![n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 是特征的个数）。这样一个矩阵求逆的运算复杂度大约在 [![O(n^{2.4})](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-31ba2ac7dbc438cef695358d6e49deb3.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-31ba2ac7dbc438cef695358d6e49deb3.gif) 到 [![O(n^3)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-4a7d22b39e93fbbcbe107e7a19e8bd34.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-4a7d22b39e93fbbcbe107e7a19e8bd34.gif) 之间，具体值取决于计算方式。换句话说，如果你将你的特征个数翻倍的话，其计算时间大概会变为原来的 5.3（[![2^{2.4}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-61f8abcb13be8d0a51b2868de491d3a8.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-61f8abcb13be8d0a51b2868de491d3a8.gif)）到 8（[![2^3](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-5b9a77af89d04a685b4f649da485aed3.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-5b9a77af89d04a685b4f649da485aed3.gif)）倍。

> 提示
>
> 当特征的个数较大的时候（例如：特征数量为 100000），正态方程求解将会非常慢。

有利的一面是，这个方程在训练集上对于每一个实例来说是线性的，其复杂度为 [![O(m)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-0e2ae329177722b1818828e92b441032.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-0e2ae329177722b1818828e92b441032.gif)，因此只要有能放得下它的内存空间，它就可以对大规模数据进行训练。同时，一旦你得到了线性回归模型（通过解正态方程或者其他的算法），进行预测是非常快的。因为模型中计算复杂度对于要进行预测的实例数量和特征个数都是线性的。 换句话说，当实例个数变为原来的两倍多的时候（或特征个数变为原来的两倍多），预测时间也仅仅是原来的两倍多。

接下来，我们将介绍另一种方法去训练模型。这种方法适合在特征个数非常多，训练实例非常多，内存无法满足要求的时候使用。

## 梯度下降

梯度下降是一种非常通用的优化算法，它能够很好地解决一系列问题。梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。

假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量[![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。

具体来说，开始时，需要选定一个随机的[![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)（这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值（如图：4-3）。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-3.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-3.PNG)

梯度下降

在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-4.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-4.PNG) 

然鹅，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使得算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-5.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-5.PNG) 

最后，并不是所有的损失函数看起来都像一个规则的碗状。他们可能是洞，山脊，高原或者其他各种不规则的地形，使他们收敛到最小值非常困难，下图显示了梯度下降的两个主要挑战：如果随机初始值选在了图像的左侧，则他将收敛到局部最小值，这个值要比全局最小值要大。如果从右边开始，那么跨过高原将需要很长的时间，如果你过早结束训练吗，你将永远也得不到全局最小值。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-6.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-6.PNG) 

Trap in 梯度下降

所幸线性回归模型的均方差损失函数是一个凸函数，这意味着如果你选择曲线上的任意两点，他们之间的连线不会与曲线有第三个交点。这意味着这个损失函数没有局部最小值，即仅有一个极值（最小值）。同时他也是一个斜率不能突变的连续函数。这两个因素导致了一个很好的结果：梯度下降可以无限接近全局最小值。（只要你训练时间足够长，同时学习率‘步长’不是太大）。

事实上，损失函数的图像呈现碗状，但是不同特征的取值范围相差较大时，这个碗可能是细长的。如下图右图显示了梯度下降在不同训练集上的表现（下图的制图原理类似弟理中的等高线地形图）。在左图中，特征1和特征2有着相同的数值尺度，在右图中，特征1比特征2的取值要小的多，由于特征1较小，因此损失函数改变时，[![\theta_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif) 会有较大的变化，于是这个图像会在[![\theta_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif)轴方向变得细长。 

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-7.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-7.PNG) 

有无特征缩放的梯度下降 

如你所见，左边的梯度下降可以直接快速的到达最小值，然而在右边的梯度下降第一次前进的方向几乎和全局最小值的方向垂直，并且最后到达一个几乎平坦的山谷，在平坦的山谷走了很长时间。他最终会到达最小值，但它需要很长时间。

> 提示
>
> 当我们使用梯度下降的时候，应该确保所有的特征有着相近的尺度范围（例如：使用 Scikit Learn 的 `StandardScaler`类），否则它将需要很长的时间才能够收敛。 

这幅图也表明了一个事实：训练模型意味着找到一组模型参数，这组参数可以在训练集上使得损失函数最小。这是对于模型参数空间的搜索，模型的参数越多，参数空间的维度越多，找到合适的参数越困难。例如在300维的空间找到一枚针要比在三维空间里找到一枚针复杂得多。幸运的是线性回归模型的损失函数是凸函数，这个最优参数一定在碗底。

### 批量梯度下降（Batch gradient descent ）

使用梯度下降的过程中，你需要计算每一个 [![\theta_j](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif) 下损失函数的梯度。换句话说，你需要计算当[![\theta_j](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif)变化一点点时，损失函数改变了多少。这称为偏导数（**参考《同济大学出版社高等数学下册第七版》  **），它就像当你面对东方的时候问："我脚下的坡度是多少？"。然后面向北方的时候问同样的问题（如果你能想象一个超过三维的宇宙，可以对所有的方向都这样做）。公式 4-5 计算关于 [![\theta_j](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cb0e17d96e58d55d1eb06dc1b14b7a7b.gif) 的损失函数的偏导数，记为 [![\frac{\partial }{\partial \theta_j}MSE(\theta)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-5c3b6b00cffc9732138715003b0e557a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-5c3b6b00cffc9732138715003b0e557a.gif)。

公式 4-5： 损失函数的偏导数

[![\frac{\partial }{\partial \theta_j}MSE(\theta)=\frac{2}{m} \sum\limits_{i=1}^m{\left(\theta^T \cdot \mathbf{x}^{(i)}-y^{(i)}\right)}{x_j}^{(i)}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3a877201402d7cd2b9d3f5b726d22b24.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3a877201402d7cd2b9d3f5b726d22b24.gif)

为了避免单独计算每一个梯度，你也可以使用公式 4-6 来一起计算它们。梯度向量记为 [![\nabla_{\theta}MSE(\theta)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2046197f9491e759ad46d9ee09227c01.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2046197f9491e759ad46d9ee09227c01.gif)，其包含了损失函数所有的偏导数（每个模型参数只出现一次）。

公式 4-6：损失函数的梯度向量

[![\nabla_{\theta}MSE(\theta)= \left(\begin{matrix} \frac{\partial }{\partial \theta_0}MSE(\theta)\ \frac{\partial }{\partial \theta_1}MSE(\theta)\ \vdots \ \frac{\partial }{\partial \theta_n}MSE(\theta)\ \end{matrix}\right)=\frac{2}{m}{\mathbf{X}}^T\cdot{(\mathbf{X}\cdot\theta-y)}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a007d1162d9c4957e8336b4b10d5fda3.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a007d1162d9c4957e8336b4b10d5fda3.gif)

> 提示
>
> 在这个方程中每一步计算时都包含了整个训练集 [![\mathbf{X}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-ca340abf4b48dc6d816137fbadf58b53.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-ca340abf4b48dc6d816137fbadf58b53.gif)，这也是为什么这个算法称为批量梯度下降：每一次训练过程都使用所有的的训练数据。因此，在大数据集上，其会变得相当的慢（但是我们接下来将会介绍更快的梯度下降算法）。然而，梯度下降的运算规模和特征的数量成正比。训练一个数千数量特征的线性回归模型使用*梯度下降要比使用正态方程快的多。

一旦求得了方向是上山的梯度向量，你就可以向着相反的方向去下山。这意味着从 [![\theta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2554a2bb846cffd697389e5dc8912759.gif) 中减去 [![\nabla_{\theta}MSE(\theta)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2046197f9491e759ad46d9ee09227c01.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2046197f9491e759ad46d9ee09227c01.gif)。学习率 [![\eta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-ffe9f913124f345732e9f00fa258552e.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-ffe9f913124f345732e9f00fa258552e.gif) 和梯度向量的积决定了下山时每一步的大小，如公式 4-7。

公式 4-7：梯度下降步长

[![\theta^{(next\ step)}=\theta - \eta\nabla_{\theta}MSE(\theta)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e25626090e2c767f539550e3c02fa6c8.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e25626090e2c767f539550e3c02fa6c8.gif)

让我们看一下这个算法的应用：

```python
eta - 0.1 #学习率（步长）
n_iteraions = 1000
m =100

theta = np.random.randn(2,1) #随机初始值
for iteration in range(n_interations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta)-y)
    theta = theta - eta * gradiens
```

这不是很难，我们看一下最后的结果theta：

```python
>>> theta
array([[4.21509616],[2.77011339]])
```

正态方程表现非常好。完美求出梯度下降的参数。但是当你换一个学习率会发生什么？图 4-8 展示了使用了三个不同的学习率进行梯度下降的前 10 步运算（虚线代表起始位置）。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-8.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-8.PNG) 

不同学习率的梯度下降

在左边的那幅图中，学习率是最小的，算法几乎不能求出最后的结果，而且还会花费大量的时间，在中间这幅图中，学习率的表现看起来不错，仅仅几次迭代后，他就收敛到了最后的结果。在右边的那幅图中，学习率太大了，算法是发散的，跳过了所有的训练样本，同时每一步都离正确的结果越来越远。

为了找到一个好的学习率，你可以使用网格搜索（在回归问题中讲过）。当然，你一般会限制迭代的次数，以便网格搜索可以消除模型需要很长时间才能收敛这一问题。

你可能想知道如何选取迭代的次数。如果他太小了，当算法停止的时候，你依然没有找到最优解。如果他太大了，算法会非常的耗时间同时后来的迭代参数也不会发生改变。有一个简单的解决方案：设置一个非常大的迭代次数，但是当梯度向量变得非常小的时候，结束迭代。非常小指的是：梯度向量小于一个值 [![\varepsilon](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f8b1c5a729a09649c275fca88976d8dd.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f8b1c5a729a09649c275fca88976d8dd.gif)（称为容差）。这时候可以认为梯度下降几乎已经达到了最小值。 

> 收敛速率：
>
> 当损失函数是凸函数，同时它的斜率不能突变（就像均方差损失函数那样），那么他的批量梯度下降算法固定学习率之后，他的收敛速率是[![O(\frac{1}{iterations})](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2381690b73b9410210542c6128e83b96.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2381690b73b9410210542c6128e83b96.gif)。换句话说，如果你将容差 [![\varepsilon](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f8b1c5a729a09649c275fca88976d8dd.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f8b1c5a729a09649c275fca88976d8dd.gif) 缩小 10 倍后（这样可以得到一个更精确的结果），这个算法的迭代次数大约会变成原来的 10 倍。 

### 随机梯度下降

批量梯度下降的最重要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，他会变得非常非常慢。与之完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快，由于每一次迭代，只需要在内存中有一个实例，这使得随机梯度算法可以在大规模训练集上使用。

另一方面，由于他的随机性，与批量梯度下降相比，其呈现出更多的不规律性：他达到最小值不是平缓的下降，损失函数会忽高忽低，只是在大体上呈现下降趋势。随着时间的推移，他会非常的接近最小值，但是他不会停在一个值上，他会一直在这个值附近摆动。因此，当算法停止的时候，最后的参数还不错，但是不是最优值。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-9.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-9.PNG) 

随机梯度下降

当损失函数很不规则时，随机梯度下降算法能够跳过局部最小值。因此，随机梯度下降在寻找全局最小值上比批量梯度下降表现要好。

虽然随机性可以很好的跳过局部最优值，但同时他却不能达到全局最优值。解决一个难题的一个办法是逐渐降低学习率。开始时，走的每一步较大（这有助于快速前进同时跳过局部最小值），然后变得越来越小，从而使算法达到全局最小值。这个过程被称为模拟退火，因为它类似于熔融金属慢慢冷却的冶金学退火过程。决定每次迭代的学习率的函数成为`learning schedule`。如果学习速度降低的过快，你可能会陷入局部最小值，甚至在达到最小值的半路就停止了（比如到达高原地区）。如果学习速度降低的太慢，你可能在最小值的附近长时间摆动，同时如果过早停止训练，最终只会出现次优解。

下面的代码使用一个简单的`learning schedule`来实现随机梯度下降：

```python
n_epochs = 50
t0,t1 = 5, 50 #learnind_schedule 的超参数
def learing_schedule(t):
    return t0 / (t+t1)
theta = np.random.randn(2,1)
for epoch in range(n_epochs):
    for i in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index:random_index + 1]
        yi = y[random_index:random_index + 1]
        gradients = 2 * xi.T.dot(xi,dot(theta) - yi)
        eta = learning_schedule(epoch * m + i)
        theta = theta - eta * gradiens
```

按习惯来讲，我们进行m轮的迭代，每一轮迭代被称为一代。在整个训练集上，随机梯度下降迭代1000次时，一般在第五十次的时候就可以达到一个比较好的结果。

```python
>>> theta
array([[4.21076011],[2.748560791]])
```

下图展示了前 10 次的训练过程（注意每一步的不规则程度）。 

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-10.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-10.PNG) 

由于每个实例的选择是随机的，有的实例可能在每一代中都被选到，这样其他的实例可能一直不被选到。如果你想保证每一次迭代过程，算法可遍历所有的实例，一种方法是将训练集打乱重排，然后选择一个实例，之后再继续打乱重排，一次类推一直进行下去。但是这样收敛速度会非常慢。

通过使用 Scikit-Learn 完成线性回归的随机梯度下降，你需要使用`SGDRegressor`类，这个类默认优化的是均方差损失函数。下面的代码迭代了 50 代，其学习率 [![\eta](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-ffe9f913124f345732e9f00fa258552e.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-ffe9f913124f345732e9f00fa258552e.gif) 为0.1（`eta0=0.1`），使用默认的`learning schedule`（与前面的不一样），同时也没有添加任何正则项（`penalty = None`）： 

[关于SGD](http://d0evi1.com/sklearn/sgd/)

```python
from sklearn.linear_model import SGDRegressor
sgd_reg + SGDRregressor(n_iter=50, penalty=None, eta0=0.1)
sgd_reg.fit(X,y.ravel())
```

你可以再一次发现，这个结果非常的接近正态方程的解：

 ```python
>>> sgd_reg.intercept_, sgd_reg.coef_
(array([4.18380366]),array([2.74205299]))
 ```

### 小批量梯度下降

最后一个梯度下降算法，我们将介绍小批量梯度下降算法，一旦你理解了批量梯度下降和随机梯度下降，再去理解小批量梯度下降是非常简单的。在迭代的每一步，批量梯度使用整个训练集，随机梯度仅用一个实例，在小批量梯度下降中，他则使用一个随机的小型实例集。他比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用GPU进行运算的时候。

小批量梯度下降在参数空间上的表现比随机梯度下降要好得多，尤其是在有大量的小型实例集时。作为结果，小批量梯度下降会比随机梯度更靠近最小值。但是，另一方面，他有可能陷在局部最小值中国（在遇到局部最小值问题的情况下，和我们之前看到的线性回归不一样）。下图显示了训练期间三种梯度下降算法在参数空间中所采用的路径。他们都接近最小值，但批量梯度的路径最后停在了最小值，而随机梯度和小批量梯度最后都在最小值附近摆动。但是，不要忘记，批次梯度需要花费大量时间来完成每一步，但是，如果你使用了一个较好的`learning schedule`，随机梯度和小批量梯度也可以得到最小值。 

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-11.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-11.PNG) 

参数空间的梯度下降路径

让我们比较一下目前我们已经探讨过的对线性回归的梯度下降算法。如表 4-1 所示，其中 m 表示训练样本的个数，n 表示特征的个数。

比较线性回归的不同梯度下降算法

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E8%A1%A84-1.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E8%A1%A84-1.PNG) 

>提示
>
>上述算法在完成训练后，得到的参数没什么不同，他们会得到非常相似的模型，最后会以一样的方式去进行预测。

### 多项式回归

如果你的数据实际上比简单的直线更复杂呢？令人惊讶的是，你依然可以使用线性模型来拟合非线性数据。一个简单的方法是对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。这种方法称为多项式回归。

让我们看一个例子。首先，我们根据一个简单的二次方程（并加上一些噪声）来生成一些非线性数据：

```python
m = 100
X = 6 * np.random.rand(m,1) - 3 
y = 0.5*X**2+X+2+np.random.randn(m,1)
```

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-12.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-12.PNG) 

很清楚的看出，直线不能恰当的拟合这些数据。于是，我们使用Skikit-Learning的`PolynomialFeatures`类进行训练数据集的转换，让训练集中每个特征的平方（2次多项式）作为新特征（在这种情况下，仅存在一个特征）：

```python
>>> from sklearn.preprocessing import PolynomialFeatures
>>> poly_features = PolynomialFeatures(degree=2,include_bias=False)
>>> X_poly = poly_features.fit_transform(X)
>>> X[0]
array([-0.75275929])
>>> X_poly[0]
array([-0.75275929, 0.56664654])
```

`X_poly`现在包含了原始特征`X`并加上了这个特征的平方 [![X^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-75299c2520ca389119694b3da7cc7a84.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-75299c2520ca389119694b3da7cc7a84.gif)。现在你可以在这个扩展训练集上使用`LinearRegression`模型进行拟合，如图 4-13： 

```python
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X_poly, y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([ 1.78134581]), array([[ 0.93366893, 0.56456263]]))
```

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-13.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-13.PNG) 

多项式回归模型预测

还是不错的，模型预测函数 [![\hat{y}=0.56x_1^2+0.93x_1+1.78](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-24ff843f62db9062b4bb1fba4040c10f.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-24ff843f62db9062b4bb1fba4040c10f.gif)，事实上原始函数为 [![y=0.5x_1^2+1.0x_1+2.0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-6f58e3449c2d62d9634b79c0484c14ac.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-6f58e3449c2d62d9634b79c0484c14ac.gif) 再加上一些高斯噪声。 

请注意，当存在多个特征时，多项式回归能够找出特征之间的关系（这是普通线性回归模型无法做到的）。这是因为`LinearRegression`会自动添加当前阶数下特征的所有组合。例如，如果有两个特征 [![a,b](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-b345e1dc09f20fdefdea469f09167892.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-b345e1dc09f20fdefdea469f09167892.gif)，使用 3 阶（`degree=3`）的`LinearRegression`时，不仅有 [![a^2,a^3,b^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e2319eb828681ba30bf7e05e07d7f5fa.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e2319eb828681ba30bf7e05e07d7f5fa.gif) 以及 [![b^3](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2d275b176c3436e8981c70371f474c9c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2d275b176c3436e8981c70371f474c9c.gif)，同时也会有它们的其他组合项 [![ab,a^2b,ab^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e373951eded9d8d1fddb4db810c5069f.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e373951eded9d8d1fddb4db810c5069f.gif)。 

>提示
>
>`PolynomiaFeatures(degree = d)`把一个包含 n 个特征的数组转换为一个包含 [![\frac{(n+d)!}{d!n!}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-bbbca49c7d6afb0938a5c37bb5b5fbcf.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-bbbca49c7d6afb0938a5c37bb5b5fbcf.gif) 特征的数组，[![n!](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-388f554901ba5d77339eec8b26beebea.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-388f554901ba5d77339eec8b26beebea.gif) 表示 [![n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7b8b965ad4bca0e41ab51de7b31363a1.gif) 的阶乘，等于 [![1 * 2 * 3 \cdots * n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-62b4db05802cc87cf8ed00845ce751af.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-62b4db05802cc87cf8ed00845ce751af.gif)。小心大量特征的组合爆炸！ 

### 学习曲线

如果你使用一个高阶的多项式回归，你可能发现他的拟合程度要比普通的线性回归要好得多。例如，下图使用一个 300 阶的多项式模型去拟合之前的数据集，并同简单线性回归/2阶的多项式回归进行比较。注意 300 阶的多项式模型如何摆动以尽可能接近训练实例。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-14.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-14.PNG) 

高阶多项式回归

当然，这种高阶多项式回归模型在这个训练集上严重过拟合了，线性模型则欠拟合。在这个训练集上，二次模型有较好的泛化能力。那是因为在生成数据时使用了二次模型，但是一般我们不知道这个数据生成函数是什么，那我们该决定我们模型的复杂度呢？你如何告诉我你的模型是过拟合还是欠拟合？

在第二章，你可以使用交叉验证来估计一个模型的泛化能力。如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了。这种方法可以告诉我们，你的模型是太复杂了还是太简单了。

另一种方法是观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模自己上进行多次训练。下面的代码定义了一个函数，用来画出给定训练集后的模型学习曲线：

```python
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

def plot_learning_curves(model ,X ,y):
    X_train,X_val,y_train,y_val = train_test_split(X,y,tets_size = 0.2)
    train_errors,val_errors = [],[]
    for m in range (1,len(X_train)):
        model.fit(X_train[:m],y_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train_predict,y_train[:m]))
        val_errors.append(mean_squared_error(y_val_predict,y_val))
 
plt.plot(np.sqrt(train_errors),"r-+",linewidth = 2,label = "train")
plt.plot(np.sqrt(val_errors),"b-"linewidth = 3,label = "val")
```

我们一起看一下简单线性回归模型的学习曲线：

```python
lin_reg = LinearRegression()
plot_learning_curves(lin_reg,X,y)
```

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-15.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-15.PNG) 

学习曲线

这幅图值得我们深究。首先，我们观察训练集的表现：当训练集只有一两个样本的时候，模型能够非常好的拟合他们，这也是为什么曲线是从零开始的原因。但是当加入了一些新的样本的时候，训练机上的拟合程度变得难以接受，出现这种情况有两个原因，一视因为数据中含有噪声，另一个是数据根本不是线性的。因此随着数据规模的增大，误差也会一直增大，直到达到高原地带并趋于稳定，在之后，继续加入新的样本，模型不能恰当的泛化，也就是为什么验证误差一开始是非常大的，当训练样本变多的时候，模型学习的东西变多，验证误差开始缓慢下降。但是一条直线不可能很好的拟合这些数据，因此最后误差会到达在一个高原地带并趋于稳定，最后和训练集的曲线非常接近。

上面的曲线表现了一个典型的欠拟合模型，两条曲线都到达高原地带并趋于稳定，并且最后两条曲线非常接近，同时误差值非常大。

>提示
>
>如果你的模型在训练集上是欠拟合的，添加更多的样本是没用的。你需要使用一个更复杂的模型或者找到更好的特征。

现在让我们看一个在相同数据上 10 阶多项式模型拟合的学习曲线：

```python
from sklearn.pipeline import Pipeline
polynomial_regression = Pipeline((
	("poly_features",PolynomialFeatures(degress = 10 ,include_bias = False)),
	("sgd_reg",LinearRegression()),
))
plot_learning_curves(polynomial_regression,X,y)
```

这幅图像和之前的有一点点像，但是其有两个非常重要的不同点：

* 在训练集上，误差要比线性回归模型低得多。
* 图中的两条曲线之间有间隔，这意味着模型在训练集上的表现要比验证集上好得多，这也是模型过拟合的显著特点。当然，如果你使用了更大的训练数据，这两条曲线最后会非常的接近。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-16.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-16.PNG) 

多项式模型的学习曲线

>提示
>
>改善模型过拟合的一种方法是提供更多的训练数据，知道训练误差和验证误差相等。

> 偏差和方差的权衡
>
> 在统计和机器学习领域有个重要的理论：一个模型的泛化误差由三个不同误差的和决定：
>
> * 偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。
> * 方差：这部分误差是由于模型对训练数据的微笑变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式），因此会导致模型过拟合。
> * 不可约误差：这部分误差是由于数据本身的噪声决定的

### 线性模型的正则化

正如我们在第一和第二章看到的那样，降低模型的过拟合的好方法是正则化这个模型（即限制他）：模型有越少的自由度，就越难以拟合。例如，正则化一个多项式模型，一个简单的方法就是减少多项式的阶数。

对于一个线性模型，正则化的典型实现就是约束模型中参数的权重。接下来我们将介绍三种不同约束权重的方法：Ridge回归。Lasso回归和Elastic Net。

#### 岭（Ridge）回归

岭回归（也称为Tikhonov正则化）是线性回归的正则化版：在损失函数上直接加上一个正则项[![\alpha\sum_{i=1}^n\theta_i^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a21c05b05e3a61cef53414437bae86cf.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a21c05b05e3a61cef53414437bae86cf.gif)。 这使得学习算法不仅能够拟合数据，而且能够使模型的参数权重尽量的小。注意到这个正则项只有在训练过程中才会被加到损失函数。当得到完成训练的模型后，我们应该使用没有正则化的测量方法去评价模型的表现。

> 提示
>
> 一般情况下，训练过程使用的损失函数和测试过程使用的评价函数是不一样的。除了正则化，还有一个不同：训练时的损失函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现。一个好的例子：在分类训练中我们使用对数损失作为损失函数，但是我们却使用精确率/召回率来作为他的评价函数。

超参数 α 决定了你想正则化这个模型的强度。如果 α = 0 那此使的岭回归便变味了线性回归。如果 α 非常的大，所有的权重最后都接近于零，最后结果将是一条穿过数据平均值的水平直线。下式是岭回归的损失函数：

[![J(\theta)=MSE(\theta)+\alpha\frac{1}{2}\sum\limits_{i=1}^n\theta_i^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-de03ddd330336d12e33df21217bdab9d.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-de03ddd330336d12e33df21217bdab9d.gif) 

值得注意的是偏差偏[![\theta_0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-dbc9011a370bca098d4752346ba71d5c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-dbc9011a370bca098d4752346ba71d5c.gif) 是没有被正则化的（累加运算的开始是 i = 1 而不是 i = 0）.如果我们定义 w  作为特征的权重向量（[![\theta_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7672d625e9a2492987c50d3b87c04349.gif) 到 [![\theta_n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a3026e320c132de94f7c8ebb952bda60.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a3026e320c132de94f7c8ebb952bda60.gif) ） 那么正则项可以简写成 [![\frac{1}{2}{({\parallel \mathbf{w}\parallel_2})}^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e236013a1d5cfb056aa71c770d62e4ed.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e236013a1d5cfb056aa71c770d62e4ed.gif)，其中 [![\parallel \cdot \parallel_2 ](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-3fd16a45c4fce610740da450e9f5a283.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-3fd16a45c4fce610740da450e9f5a283.gif) 表示权重向量的 [![\ell_2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数。对于梯度下降来说仅仅在均方差梯度向量（公式 4-6）加上一项 [![\alpha\mathbf{w}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-b0bb6cfa6e49912f2da3f807dd931480.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-b0bb6cfa6e49912f2da3f807dd931480.gif)。 

> 提示
>
> 在使用岭回归前，对数据进行放缩（可以使用`StandarScaler`）是非常重要的，算法对于输入的特征的数值尺度（scale）非常敏感。大多数的正则化模型都是这样的。

下图展示了在相同线性数据上使用不同 [![\alpha](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif) 值 的岭回归模型最后的表现。左图中，使用简单的岭回归模型，最后得到了线性的预测。右图中的数据首先使用 10 阶的`PolynomialFearures`进行拓展，然后使用`StandardScaler`进行缩放，最后将岭模型应用在处理过后的特征上。这就是带有岭正则项的多项式回归。注意当[![\alpha](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7b7f9dbfea05c83784f8b85149852f08.gif)增大的时候，导致预测曲线变得扁平（即少了极端值，多了一般值），这样减少了模型的方差，却增加了模型的偏差。 

对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理。他们的优缺点是一样的。公式 4 - 9 表示封闭方程的解（矩阵 [![\mathbf{A}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-6c6404adc033dfed51422fdaf7fa0494.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-6c6404adc033dfed51422fdaf7fa0494.gif) 是一个除了左上角有一个 [![0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cfcd208495d565ef66e7dff9f98764da.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cfcd208495d565ef66e7dff9f98764da.gif) 的 [![n \times n](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-50f17e5c11d610b19c0471830dc4dda1.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-50f17e5c11d610b19c0471830dc4dda1.gif) 的单位矩，这个 [![0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cfcd208495d565ef66e7dff9f98764da.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cfcd208495d565ef66e7dff9f98764da.gif) 代表偏差项。译者注：偏差 [![\theta_0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-dbc9011a370bca098d4752346ba71d5c.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-dbc9011a370bca098d4752346ba71d5c.gif) 不被正则化的）。 

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-17.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-17.PNG) 

岭回归的封闭方程的解

[![\hat{\theta} = ({\mathbf{X}}^T\cdot\mathbf{X}+\alpha\mathbf{A})^{-1}\cdot{\mathbf{X}}^T\cdot\mathbf{y}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2ef1c91a9cc8eeb7da8227d4016d702e.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2ef1c91a9cc8eeb7da8227d4016d702e.gif) 

下面是如何使用 Scikit -Learn 来进行封闭方程的求解（使用Cholesky法进行矩阵分解对公式 4 - 9 进行变形）：

```python
>>> from sklearn.linear_model import Ridge
>>> ridge_reg = Ridge(alpha=1, solver="cholesky")
>>> ridge_reg.fit(X, y)
>>> ridge_reg.predict([[1.5]])
array([[ 1.55071465]]
```

使用随机梯度法进行求解：

```python
>>> sgd_reg = SGDRegressor(penalty="l2")
>>> sgd_reg.fit(X, y.ravel())
>>> sgd_reg.predict([[1.5]])
array([[ 1.13500145]])
```

`penalty`参数指的是正则项的惩罚类型。制定“l2”表明你要在损失函数上添加一项：权重向量 [![\ell_2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数平方的一半，这就是简单的岭回归。 

### Lasso回归

Lasso回归（也称Least Absolute Shrinkage，或者Selection Operator Regression）是另一种正则化版的线性回归：就像岭回归那样，他也在损失函数上添加了一个正则化项，但是他使用权重向量的 [![\ell_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif) 范数而不是权重向量 [![\ell_2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 范数平方的一半。（如公式 4-10）

公式4 - 10 ：Lasso回归的损失函数

[![J(\theta)=MSE(\theta)+\alpha\sum\limits_{i=1}^n\left|\theta_i \right|](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a78e85b9c0eb6446f86c17d6d2190b74.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a78e85b9c0eb6446f86c17d6d2190b74.gif) 

下图展示了和岭回归相同的事情仅仅是用 Lasso 模型代替了 Ridge 模型，同时调小了 α 的值。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-18.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-18.PNG)

Lasso 回归

Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将他们设置为0）.例如，右图中的虚线所示（[![\alpha=10^{-7}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-af5e712113f834592672c1a7e4f1bee2.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-af5e712113f834592672c1a7e4f1bee2.gif)），曲线看起来像一条二次曲线，而且几乎是线性的，这是因为所有的高阶多项特征都被设置为0.换句话说，Lasso 回归自动的进行特征选择同时输出一个稀疏模型（即，具有很少的非零权重）。

你可以从下图知道为什么会出现这种情况：在左上角图中，后背景的等高线（椭圆）表示了没有正则化的均方差损失函数（[![\alpha=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-2aa447f3144cccc2865e6268c583f0f3.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-2aa447f3144cccc2865e6268c583f0f3.gif)），白色的小圆圈表示在当前损失函数上批量梯度下降的路径。前背景的等高线（菱形）表示[![\ell_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif)惩罚，黄色的三角形表示了仅在这个惩罚下批量梯度下降的路径（[![\alpha\rightarrow\infty](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-7c44c4fd9ee64f79d37dc97e3ceb3c17.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-7c44c4fd9ee64f79d37dc97e3ceb3c17.gif)）。注意路径第一次是如何到达 [![\theta_1=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-224ba64bbd16cef44085c714ff69b794.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-224ba64bbd16cef44085c714ff69b794.gif)，然后向下滚动直到它到达 [![\theta_2=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-421570724060381e95985593de9d77c9.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-421570724060381e95985593de9d77c9.gif)。在右上角图中，等高线表示的是相同损失函数再加上一个 [![\alpha=0.5](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-d7e489c9bd21eb7274ea7acd2b4f6b5b.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-d7e489c9bd21eb7274ea7acd2b4f6b5b.gif) 的 [![\ell_1](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-8524eb1789cf2093cfccc4c297138c7f.gif) 惩罚。这幅图中，它的全局最小值在 [![\theta_2=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-421570724060381e95985593de9d77c9.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-421570724060381e95985593de9d77c9.gif)这根轴上。批量梯度下降首先到达 [![\theta_2=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-421570724060381e95985593de9d77c9.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-421570724060381e95985593de9d77c9.gif)，然后向下滚动直到达到全局最小值。 两个底部图显示了相同的情况，只是使用了 [![\ell_2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-f2d02eaf32cb7a351989198531c0d12a.gif) 惩罚。 规则化的最小值比非规范化的最小值更接近于 [![\theta=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-cbc3c4cd0071f0ac61b8ce488ff05234.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-cbc3c4cd0071f0ac61b8ce488ff05234.gif)，但权重不能完全消除。

[![img](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/chapter_4/%E5%9B%BE4-19.PNG)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/chapter_4/%E5%9B%BE4-19.PNG)

Ridge 回归和 Lasso 回归对比

> 提示
>
> 在Lasso损失函数中，批量梯度下降的路径趋向在低谷有一个反弹。这是因为在  [![\theta_2=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-421570724060381e95985593de9d77c9.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-421570724060381e95985593de9d77c9.gif) 时斜率会有一个突变。为了最后真正收敛到全局最小值，你需要逐渐地降低学习率。

Lasso 损失函数在[![\theta_i=0(i=1,2,\cdots,n) ](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e4df33b5fe9a1d3789867afe58c9564a.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e4df33b5fe9a1d3789867afe58c9564a.gif) 处无法进行微分运算，但是梯度下降如果你使用子梯度向量 [![\mathbf{g}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-d324bced968eda52e62e58cb90c82c2d.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-d324bced968eda52e62e58cb90c82c2d.gif) 后它可以在任何 [![\theta_i=0](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-a64a4a0de329e98ac7b25d532cd74a4d.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-a64a4a0de329e98ac7b25d532cd74a4d.gif) 的情况下进行计算。公式 4-11 是在 Lasso 损失函数上进行梯度下降的子梯度向量公式。

Lasso 回归子梯度向量



[![g(\theta,J)=\nabla_{\theta}MSE(\theta)+ \alpha{\left(\begin{matrix} sign(\theta_1)\ sign(\theta_2)\ \vdots \ sign(\theta_n)\ \end{matrix}\right)}\ where\ sign(\theta_i)= \begin{cases} -1, &\theta_i<0 \ 0, &\theta_i=0 \ +1,&\theta_i>0 \ \end{cases}](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-93eea6b5c197bbc8d7be8b4c14e9f8f3.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-93eea6b5c197bbc8d7be8b4c14e9f8f3.gif)

下面是一个使用Scikit-Learn的Lasso类的小例子，你也可以使用`SGDRegressor(penalty="l1")`来代替他。

```python
>>> from sklearn.linear_model import Lasso
>>> lasso_reg = Lasso(alpha=0.1)
>>> lasso_reg.fit(X, y)
>>> lasso_reg.predict([[1.5]])
array([ 1.53788174]
```

### 弹性网络（ElasticNet）

弹性网络介于Ridge回归和Lasso回归之间。他的正则项是Ridge回归和Lasso回归项的简单混合，同时你可以控制他们的混合率 r ，当 r = 0 时，弹性网络就是Ridge回归，当 r = 1，时，就是Lasso回归。弹性网络损失函数公式如下：

[![J(\theta)=MSE(\theta)+r\alpha\sum\limits_{i=1}^n\left|\theta_i \right|+\frac{1-r}{2}\alpha\sum\limits_{i=1}^n\theta_i^2](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/raw/dev/images/tex-e4da079f692fe35778bbdf1fdf120d99.gif)](https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/images/tex-e4da079f692fe35778bbdf1fdf120d99.gif)

那么我们该如何选择线性回归，岭回归，Lasso回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归

















