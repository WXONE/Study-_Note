{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主题建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主题建模专门设计用于从包含各种类型文档的大型语料库中提取各种不同概念或主题，其中每个文档涉及一个或多个概念。这些概念可以是从思想到一件、事实、展望、陈述等。主题建模的主要目的是使用数学和统计技术来发现语料库中的隐藏和潜在语义结构。\n",
    "主题建模涉及从文档词项中提取特征，并使用矩阵分解和SVD等数学结构和框架来生成彼此不同的词簇或词组，并且这些词簇形成主题或概念。\n",
    "构建主题模型有各种框架和算法。我们将介绍以下三种方法：\n",
    "* 隐含语义索引\n",
    "* 隐含Dirichlet分布。\n",
    "* 非负矩阵分解\n",
    "我们将使用gensim和scikit-learn来进行实际的实现，并且还会介绍如何基于隐含语义索引来构建自己的主题模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from normalization3 import normalize_corpus\n",
    "import numpy as np\n",
    "\n",
    "toy_corpus = [\"The fox jumps over the dog\",\n",
    "\"The fox is very clever and quick\",\n",
    "\"The dog is slow and lazy\",\n",
    "\"The cat is smarter than the fox and the dog\",\n",
    "\"Python is an excellent programming language\",\n",
    "\"Java and Ruby are other programming languages\",\n",
    "\"Python and Java are very popular programming languages\",\n",
    "\"Python programs are smaller than Java programs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库中共八个文档，前四个是关于动物的，后四个是关于编程语言的。一旦构建了一些主题建模框架，我们将使用相同的方式来生成原子亚马逊实际产品评论的主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含语义索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function normalize_corpus at 0x0000022A2A51DC80>\n"
     ]
    }
   ],
   "source": [
    "norm_tokenized_corpus = normalize_corpus(toy_corpus,tokenize=True)\n",
    "print(normalize_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 0, 'fox': 1, 'jump': 2, 'clever': 3, 'quick': 4, 'lazy': 5, 'slow': 6, 'cat': 7, 'smarter': 8, 'excellent': 9, 'language': 10, 'programming': 11, 'python': 12, 'java': 13, 'ruby': 14, 'popular': 15, 'program': 16, 'small': 17}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "\n",
    "print (dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (1, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(10, 1), (11, 1), (13, 1), (14, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (15, 1)],\n",
       " [(12, 1), (13, 1), (16, 2), (17, 1)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在将对这个语料库建立一个TF-IDF加权模型，其中每个文档中的每个词将包含其TF-IDF权重。这类似于特征提取或向量空间转换，其中每个文档由其词的TF-IDF向量表示。完成之后我们将在这些特征上构建一个LSI模型，并输入我们想要生成的主题数量。这个数字是基于直觉和试错，所以在语料库上建立主题模型时，以可随意尝试这个参数。根据我们期望小语料库所包含的主题数量将此参数设置为2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "total_topics = 2\n",
    "lsi = models.LsiModel(corpus_tfidf,\n",
    "                     id2word = dictionary,\n",
    "                     num_topics = total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "-0.459*\"language\" + -0.459*\"programming\" + -0.344*\"python\" + -0.344*\"java\" + -0.336*\"popular\" + -0.318*\"excellent\" + -0.318*\"ruby\" + -0.148*\"program\" + -0.074*\"small\" + 0.000*\"fox\"\n",
      "Topic #2\n",
      "-0.459*\"fox\" + -0.459*\"dog\" + -0.444*\"jump\" + -0.322*\"smarter\" + -0.322*\"cat\" + -0.208*\"clever\" + -0.208*\"quick\" + -0.208*\"lazy\" + -0.208*\"slow\" + -0.000*\"programming\"\n"
     ]
    }
   ],
   "source": [
    "for index,topic in lsi.print_topics(total_topics):\n",
    "    print('Topic #'+ str(index + 1))\n",
    "    print(topic)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数有助于在有阙值或无阙值的情况下以更好的方式显示主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics_gensim(topic_model,total_topics = 1,\n",
    "                       weight_threshold = 0.0001,\n",
    "                       display_weights = False,\n",
    "                       num_terms = None):\n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index)\n",
    "        topic = [(word,round(wt,2))for word,wt in topic \n",
    "                if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #' + str (index+1)+'with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #' + str(index+1)+'without weights')\n",
    "            tw = [term for term,wt in topic]\n",
    "            print(tw[:num_terms])if num_terms else tw\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用以下代码对小语料库的主题模型测试这个函数，以了解如何获取主题并调整参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1without weights\n",
      "['language', 'programming', 'python', 'java', 'popular']\n",
      "Topic #2without weights\n",
      "['fox', 'dog', 'jump', 'smarter', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(topic_model=lsi,\n",
    "                   total_topics = total_topics,\n",
    "                   num_terms = 5,\n",
    "                   display_weights = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1with weights\n",
      "[('language', -0.46), ('programming', -0.46), ('python', -0.34), ('java', -0.34), ('popular', -0.34)]\n",
      "Topic #2with weights\n",
      "[('fox', -0.46), ('dog', -0.46), ('jump', -0.44), ('smarter', -0.32), ('cat', -0.32)]\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(topic_model=lsi,\n",
    "                   total_topics = total_topics,\n",
    "                   num_terms = 5,\n",
    "                   display_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经成功的使用LSI构建了一个主题建模框架，它可以从文档语料库中区分和显示主题。\n",
    "现在我们使用SVD从头开始构建自己的LSI主题模型框架。我们首先建立一个TF-IDF特征矩阵，实际上是一个文档-词项矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils_in_NLP import build_feature_matrix,low_rank_svd\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type = 'tfidf')\n",
    "td_matrix = tfidf_matrix.transpose()\n",
    "td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "total_topics = 2\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成后使用low_rank_svd()函数计算我们的词项-文档矩阵的SVD，以便我们构建一个只取前k个奇异向量的低秩矩阵逼近，这将等于我们在此情况下的主题数量。通过是用S和U分量，我们将他们一起相乘以生成每个主题的每个词频及其权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u , s , vt = low_rank_svd(td_matrix,singular_count=total_topics)\n",
    "weights = u.transpose() * s[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了词项的权重，需要将他们连接回到我们的词项。我们定义两个效用函数，用于通过连接词项与权重来生成这些主题，然后使用具有可配置参数的函数来打印这些主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) \n",
    "                           for row \n",
    "                           in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) \n",
    "                               for wt, index \n",
    "                               in zip(weights,sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) \n",
    "                             for row \n",
    "                             in sorted_indices])\n",
    "    \n",
    "    topics = [np.vstack((terms.T, \n",
    "                     term_weights.T)).T \n",
    "              for terms, term_weights \n",
    "              in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        topic = [(word, round(wt,2)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "                     \n",
    "        if display_weights:\n",
    "            print ('Topic #'+str(index+1)+' with weights')\n",
    "            print (topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print ('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print( tw[:num_terms]) if num_terms else tw\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics = topics,\n",
    "                total_topics = total_topics,\n",
    "                weight_threshold = 0,\n",
    "                display_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(\n",
    "                topics = topics,\n",
    "                total_topics = total_topics,\n",
    "                weight_threshold = 0.15,\n",
    "                display_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用LSI来定义下面的函数作为通用可重用的主题建模框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lsi_model_gensim(corpus,total_topics = 2):\n",
    "    norm_tokenized_corpus = normalize_corpus(corpus,tokenize = True)\n",
    "    dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "    mapped_corpus = [dictionary.doc2bow(text)\n",
    "                    for text in norm_tokenized_corpus]\n",
    "    tfidf = models.TfidfModel(mapped_corpus)\n",
    "    corpus_tfidf = tfidf[mapped_corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf,\n",
    "                         id2word = dictionary,\n",
    "                         num_topics = total_topics)\n",
    "    return lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含Dirichlet(狄利克利)分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "隐含Dirichlet分布技术是一种概率生成模型，其中假定每个文档具有类似于概率隐含语义索引模型的主题组合--但是在此情况下，隐含主题包含他们的Dirichlet先验分布。这项技术背后的数学知识比较复杂，因为他的具体细节将超出当前范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假设我们有M个文档，N个文档中的单词，以及K个想要生成的主题数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法思想：\n",
    "1 初始化必要参数。\n",
    "2 对于每个文档，随机将每个单词初始化为K个主题之一。\n",
    "3 开始如下的一个迭代过程，重复几次。\n",
    "4 对于每个文档D：\n",
    "a 对于文档中的每个单词W：\n",
    "* 对于每个主题T\n",
    "* 计算P(T|D),其是D中分配给主题T的词的比例\n",
    "* 计算P(W|D),其是对于含有词W的所有文档分配给主题T的比例。\n",
    "* 考虑所有其他单词及其主题分配，用主题T和概率P(T|D)*P(W|D)重新分配词W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行了几次迭代之后，我们应该为每个文档提供主题混合，然后从指向该主题的词中生成每个主题的组成部分。我们在以下实现中使用gensim来构建基于LDA的主题模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda_model_gensim(corpus,total_topics = 2):\n",
    "    norm_tokenized_corpus = normalize_corpus(corpus,tokenize = True)\n",
    "    dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "    mapped_corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n",
    "    tfidf = models.TfidfModel(mapped_corpus)\n",
    "    corpus_tfidf = tfidf[mapped_corpus]\n",
    "    lda = models.LdaModel(corpus_tfidf,\n",
    "                         id2word = dictionary,\n",
    "                         iterations = 1000,\n",
    "                         num_topics = total_topics)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1with weights\n",
      "[('fox', 0.07), ('quick', 0.07), ('cat', 0.07), ('clever', 0.07), ('dog', 0.06)]\n",
      "Topic #2with weights\n",
      "[('programming', 0.07), ('language', 0.07), ('java', 0.07), ('popular', 0.06), ('jump', 0.06)]\n"
     ]
    }
   ],
   "source": [
    "lda_gensim = train_lda_model_gensim(toy_corpus,\n",
    "                                   total_topics = 2)\n",
    "print_topics_gensim(topic_model = lda_gensim,\n",
    "                   total_topics = 2,\n",
    "                   num_terms = 5,\n",
    "                   display_weights  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type='tfidf')\n",
    "total_topics = 2\n",
    "lda = LatentDirichletAllocation(n_topics = total_topics,\n",
    "                               max_iter = 100,\n",
    "                               learning_method = 'online',\n",
    "                               learning_offset = 50,\n",
    "                               random_state = 42)\n",
    "lda.fit(tfidf_matrix)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = lda.components_\n",
    "topics = get_topics_terms_weights(weights,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该段代码中，将LDA模型应用于文档-词项的TF-IDF特征矩阵，其被分解成两个矩阵，即一个文档-主题矩阵和一个主题-词项矩阵。我们使用存储在lda.components_ 中的主题-词项来检索每个主题每个词的权重。得到这些权重后，我们使用LSI建模中的get_topics_terms_weights()函数根据每个主题的词项和权重来构建主题。我们现在可以使用之前实现的print_topics_udf()函数查看主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('fox', 1.85), ('dog', 1.54), ('jump', 1.17), ('clever', 1.11), ('quick', 1.11), ('cat', 1.06), ('smarter', 1.05), ('excellent', 0.6)]\n",
      "Topic #2 with weights\n",
      "[('programming', 1.73), ('language', 1.73), ('java', 1.61), ('python', 1.58), ('program', 1.29), ('ruby', 1.09), ('slow', 1.08), ('lazy', 1.08)]\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics = topics,\n",
    "                 total_topics = total_topics,\n",
    "                num_terms = 8,\n",
    "                display_weights = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非负矩阵分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非负矩阵分解(NNMF)是一种类似于SVD的矩阵分解技术，虽然NNMF是对非负矩阵操作运算，并也可适用于多变量数据。NNMF可定义为：给定非负矩阵V，目标是找到两个非负矩阵因子W和H，使得他们相乘时，他们可以近似重构V，数学上这表示为V≈W * H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使得所有三个矩阵都为非负。为了实现这个近似，我们通常使用一个成本函数，如两个矩阵之间的欧几里得距离是L2范数，或是L2范数略微修改的Frobenius范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从scikit-learn decomposition模块的NMF类中可获得该实现。\n",
    "可以在我们的小语料库上使用以下代码构建一个基于NNMF的主题模型，他给出了与LDA一样的特征名称和权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type='tfidf')\n",
    "total_topics = 2\n",
    "nmf = NMF(n_components = total_topics,\n",
    "         random_state = 42,alpha = .1,l1_ratio = .5)\n",
    "nmf.fit(tfidf_matrix)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了词项及其权重，可以使用我们以前定义的函数来打印主题，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                total_topics = total_topics,\n",
    "                num_terms = None,\n",
    "                display_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从产品评论中提取主题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将获取上古卷轴的在亚马逊上的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I base the value of a game on the amount of enjoyable gameplay I can get out of it and this one was definitely worth the price!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CORPUS = pd.read_csv('amazon_skyrim_reviews.csv')\n",
    "CORPUS = np.array(CORPUS['Reviews'])\n",
    "print(CORPUS[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I base the value of a game on the amount of enjoyable gameplay I can get out of it and this one was definitely worth the price!\n",
      "Topic #1without weights\n",
      "['one', 'love', 'fun', 'buy', 'recommend', 'ever', 'quest', 'play', 'great', 'make']\n",
      "Topic #2without weights\n",
      "['much', 'fun', 'say', 'love', 'play', 'oblivion', 'great', 'thing', 'like', 'enjoy']\n",
      "Topic #3without weights\n",
      "['great', 'would', 'play', 'one', 'best', 'everyone', 'love', 'rpgs', 'like', 'skyrim']\n",
      "Topic #4without weights\n",
      "['level', 'skyrim', 'good', 'quest', 'get', 'much', 'armor', 'rpg', 'want', 'make']\n",
      "Topic #5without weights\n",
      "['play', 'skyrim', 'get', 'oblivion', 'hour', '5', 'even', 'like', 'lose', 'save']\n",
      "Topic #1without weights\n",
      "['play', 'best', 'definitely', 'say', 'oblivion', 'one', 'good', 'really', 'like', 'love']\n",
      "Topic #2without weights\n",
      "['love', 'play', 'skyrim', 'best', 'much', 'oblivion', 'elder', 'scroll', 'recommend', 'ever']\n",
      "Topic #3without weights\n",
      "['quest', 'dragon', 'skyrim', 'try', 'like', 'oblivion', 'one', 'great', 'really', 'make']\n",
      "Topic #4without weights\n",
      "['fun', 'great', 'time', 'get', 'long', 'one', 'review', 'write', 'quest', 'lot']\n",
      "Topic #5without weights\n",
      "['explore', 'quest', 'good', 'dragon', 'skyrim', 'go', 'make', 'play', 'hour', 'find']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 without weights\n",
      "['estatic', 'booklet', 'wonder4ful', 'electricity', 'heat', 'trhats', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #2 without weights\n",
      "['game', 'play', 'get', 'one', 'skyrim', 'great', 'like', 'time', 'quest', 'much']\n",
      "Topic #3 without weights\n",
      "['de', 'crédito', 'pagar', 'momento', 'compras', 'responsabilidad', 'para', 'recomiendo', 'futuras', 'skyrimseguridad']\n",
      "Topic #4 without weights\n",
      "['booklet', 'estatic', 'wonder4ful', 'electricity', 'heat', 'trhats', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #5 without weights\n",
      "['estatic', 'booklet', 'wonder4ful', 'electricity', 'trhats', 'heat', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #1 without weights\n",
      "['game', 'get', 'skyrim', 'play', 'time', 'quest', 'like', 'one', 'go', 'much']\n",
      "Topic #2 without weights\n",
      "['game', 'recommend', 'love', 'great', 'highly', 'play', 'wonderful', 'like', 'would', 'graphic']\n",
      "Topic #3 without weights\n",
      "['scroll', 'elder', 'series', 'always', 'love', 'pass', 'franchise', 'buy', 'game', 'far']\n",
      "Topic #4 without weights\n",
      "['ever', 'best', 'game', 'play', 'rpg', 'one', 'ive', 'hour', 'great', 'definitely']\n",
      "Topic #5 without weights\n",
      "['fun', 'game', 'much', 'graphic', 'improvement', 'mission', 'expect', 'see', 'hour', 'couple']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "                 \n",
    "CORPUS = pd.read_csv('amazon_skyrim_reviews.csv')                     \n",
    "CORPUS = np.array(CORPUS['Reviews'])\n",
    "\n",
    "# view sample review\n",
    "print(CORPUS[12])\n",
    "\n",
    "        \n",
    "total_topics = 5\n",
    "        \n",
    "lsi_gensim = train_lda_model_gensim(CORPUS,\n",
    "                                    total_topics=total_topics)\n",
    "print_topics_gensim(topic_model=lsi_gensim,\n",
    "                    total_topics=total_topics,\n",
    "                    num_terms=10,\n",
    "                    display_weights=False) \n",
    "\n",
    "lda_gensim = train_lda_model_gensim(CORPUS,\n",
    "                                    total_topics=total_topics)\n",
    "print_topics_gensim(topic_model=lda_gensim,\n",
    "                    total_topics=total_topics,\n",
    "                    num_terms=10,\n",
    "                    display_weights=False) \n",
    "\n",
    "\n",
    "norm_corpus = normalize_corpus(CORPUS)\n",
    "vectorizer, tfidf_matrix = build_feature_matrix(norm_corpus, \n",
    "                                    feature_type='tfidf') \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=total_topics, \n",
    "                                max_iter=1000,\n",
    "                                learning_method='online', \n",
    "                                learning_offset=10.,\n",
    "                                random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "weights = lda.components_\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 num_terms=10,\n",
    "                 display_weights=False)\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=total_topics, \n",
    "          random_state=42, alpha=.1, l1_ratio=.5)\n",
    "nmf.fit(tfidf_matrix)      \n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = nmf.components_\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 num_terms=10,\n",
    "                 display_weights=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 自动文档摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动化文档摘要的主要目标是不包括人工输入的执行此摘要，除了运行任何计算机程序。数学和统计模型有助于通过观察其内容和上下文来构建和自动化概况文档的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用自动化技术进行文档摘要主要有两大类做法：\n",
    "* 基于提取的技术：\n",
    "这些方法使用数学和统计学概念（如SVD）从原始文档中提取内容的一些关键子集，使得该内容子集包含核心信息，并作为整个文档的重点。这个内容可以是单词、短语或句子。这种方法的最终结果是从原始文档中采集或提取了几行简短的执行摘要。在这种技术中不产生新的内容--因此这个名称是基于提取的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于概括的技术：这些方法更加复杂和精准，并利用语言语义来产生表示。他们还利用NLG技术，其中及其使用知识库和语义表达来自己生成文本，并像人类编写一样来创建摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过利用gensim的摘要模块来看看文档摘要的实现。我们将使用维基百科关于大象的描述来作为我们将测试所有摘要技术的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from normalization3 import normalize_corpus,parse_document\n",
    "from utils_in_NLP import build_feature_matrix,low_rank_svd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在定义一个函数将输入文档总结道其原始大小的一小部分，这将作为下面函数中的用户输入参数summary_ratio.输出将是摘要后的文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize,keywords\n",
    "def text_summarization_gensim(text,summary_ratio = 0.5):\n",
    "    summary = summarize(text,split = True,ratio = summary_ratio)\n",
    "    for sentence in summary:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two species are traditionally recognised,  the African elephant and the Asian elephant.\n",
      "All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.\n",
      "Elephants' large ear flaps help  to control their body temperature.\n"
     ]
    }
   ],
   "source": [
    "docs = parse_document(toy_text)\n",
    "text = ' '.join(docs)\n",
    "text_summarization_gensim(text,summary_ratio=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文档共有9个句子，观察后发现，总结后共有三个句子，但是文档的核心意义与主题已被保留。\n",
    "这个原子gensim的摘要实现是基于一种流行的称为TextRank的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将主要关注以下技术：\n",
    "* 隐含语义分析\n",
    "* TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#解析和规范化文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences in Documents: 9\n"
     ]
    }
   ],
   "source": [
    "sentences = parse_document(toy_text)\n",
    "norm_sentences = normalize_corpus(sentences,lemmatize=True)\n",
    "total_sentences = len(norm_sentences)\n",
    "print('Total Sentences in Documents:',total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦有了一个可运用得摘要算法，我们将为每种技术构建一个通用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含语义分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐含语义分析（LSA）的核心原则是，在任何文件中，在词语的相关语境中存在隐含的结构，因此也应该在相同的奇异空间中相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "实现的主要思想是使用SVD，其中:$$ M = USV^T $$\n",
    "使得U和V是正交矩阵，S是对角矩阵，其也可以表示为奇异值向量。原始矩阵可以表示为词项-文档矩阵，其中行将是词，每一列将是一个文档，也就是说，在这种情况下是我们文档中的一个句子。这些值可以是任何类型的加权，例如基于词袋模型的频率、TF-IDF或出现次数二值特征。\n",
    "我们将使用low_rank_svd()函数根据概念数量k创建M的低秩矩阵近似，k将是奇异值的数量。来自矩阵U的相同的k列将指向k个概念中的每一个词向量，并且对于矩阵V，基于前k个奇异值的k行指向句子向量。从基于概念数量k的前k个奇异值的SVD中得到U、S和$V^T$之后，我们执行以下计算。\n",
    "需要的输入参数是我们预期最终摘要包含的概念数量k和句子数n。\n",
    "* 从矩阵V（k行）获取句子数量。\n",
    "* 从S获得前k个奇异值。\n",
    "* 应用基于阈值的方法，删除小于最大奇异值一半的奇异值（如果有的话）。这是启发式的，你可以按照需要调整这个值。在数学上，$S_i = 0 iff S_i <1/2 max(S)$\n",
    "* 将来自V平方的每个词句子列乘以S平方相对应的奇异值，以获得每个主题句子的凸显度分数。\n",
    "* 计算主题之间的句子权重之和，并取最终分数的平方根来获得文档中每个句子的凸显度分数。\n",
    "\n",
    "每个句子前面的凸显度分数的计算可以在数学上表示为$$SS = \\sqrt{\\sum_{i=1}^k\\ S_i*V_i^T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "其中SS表示每个句子的凸显度分数，其通过采用奇异值和$V^T$句子向量之间的点积获得。得到这些分数侯，按降序对他们进行排序，选择与最高分数相对应的前n个句子，并根据他们在原始文档中出现的顺序将他们组合起来形成最终的摘要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 3\n",
    "num_topics = 3\n",
    "vec,dt_martix = build_feature_matrix(sentences,\n",
    "                                    feature_type = 'frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = dt_martix.transpose()\n",
    "td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "u,s,vt = low_rank_svd(td_matrix,singular_count=num_topics)\n",
    "\n",
    "sv_threshold = 0.5\n",
    "min_sigma_value = max(s)*sv_threshold\n",
    "s[s<min_sigma_value] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salience_scores = np.sqrt(np.dot(np.square(s),np.square(vt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.93 3.28 1.67 1.8  2.24 4.51 0.71 1.22 5.24]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(salience_scores,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 8]\n"
     ]
    }
   ],
   "source": [
    "top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
    "top_sentence_indices.sort()\n",
    "print(top_sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two species are traditionally recognised,  the African elephant and the Asian elephant.\n",
      "Their  incisors grow into tusks, which can serve as weapons and as tools  for moving objects and digging.\n",
      "African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.\n"
     ]
    }
   ],
   "source": [
    "for index in top_sentence_indices:\n",
    "    print(sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一些矩阵操作为我们提供了一个简明，优秀的总结文档，涵盖了大象文档中的主要主题。\n",
    "我们现在使用之前的算法为LSA构建一个通用的可重用函数，以便我们可以在后续产品描述文档中使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lsa_text_summarizer(documents,num_sentences = 2,\n",
    "                       num_topics = 2,feature_type = 'frequency',\n",
    "                       sv_threshold = 0.5):\n",
    "    vec,dt_martix = build_feature_matrix(documents,feature_type=feature_type)\n",
    "    \n",
    "    td_matrix = dt_martix.transpose()\n",
    "    td_matrix = td_matrix.multiply(td_matrix>0)\n",
    "    u,s,vt = low_rank_svd(td_matrix,singular_count=num_topics)\n",
    "    min_sigma_value = max(s)*sv_threshold\n",
    "    s[s < min_sigma_value] = 0\n",
    "    salience_scores = np.sqrt(np.dot(np.square(s),np.square(vt)))\n",
    "    top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n",
    "    top_sentence_indices.sort()\n",
    "    for index in top_sentence_indices:\n",
    "        print(sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从待总结的文档中标记和提取句子\n",
    "* 确定在最终摘要中我们想要的句子数量k\n",
    "* 使用诸如TF-IDF或词袋的权重来构建文档-词项的特征矩阵\n",
    "* 通过将矩阵与其转置矩阵相乘，计算文档相似性矩阵\n",
    "* 使用这些文档作为顶点，每对文档之间的相似性作为前面提到的权重或得分系数，并将它们提供给PageRank算法\n",
    "* 获得每个句子的分数\n",
    "* 根据分数排序句子，并返回前k个句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.07 0.03 0.12 0.03 0.   0.11 0.   0.1 ]\n",
      " [0.07 1.   0.05 0.17 0.05 0.   0.07 0.   0.24]\n",
      " [0.03 0.05 1.   0.03 0.02 0.   0.03 0.   0.04]\n",
      " [0.12 0.17 0.03 1.   0.03 0.   0.11 0.   0.17]\n",
      " [0.03 0.05 0.02 0.03 1.   0.07 0.03 0.   0.04]\n",
      " [0.   0.   0.   0.   0.07 1.   0.   0.   0.  ]\n",
      " [0.11 0.07 0.03 0.11 0.03 0.   1.   0.   0.25]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.  ]\n",
      " [0.1  0.24 0.04 0.17 0.04 0.   0.25 0.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "import networkx\n",
    "num_sentences = 3\n",
    "vec,dt_martix = build_feature_matrix(norm_sentences,feature_type='tfidf')\n",
    "similarity_matrix = (dt_martix*dt_martix.T)\n",
    "print(np.round(similarity_matrix.todense(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\n",
    "networkx.draw_networkx(similarity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = networkx.pagerank(similarity_graph)\n",
    "ranked_sentences = sorted(((score,index) for index ,score in scores.items()),\n",
    "                         reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1260163898978227, 8),\n",
       " (0.11765066352817342, 1),\n",
       " (0.11552208007151823, 3),\n",
       " (0.1130880998369617, 6),\n",
       " (0.1111111111111111, 7),\n",
       " (0.1071100297193984, 0),\n",
       " (0.10529192044492774, 4),\n",
       " (0.10502675195545394, 5),\n",
       " (0.09918295343463272, 2)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "top_sentence_indices = [ranked_sentences[index][1] for index in range(num_sentences)]\n",
    "top_sentence_indices.sort()\n",
    "print(top_sentence_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two species are traditionally recognised,  the African elephant and the Asian elephant.\n",
      "Male  African elephants are the largest extant terrestrial animals.\n",
      "African elephants have larger ears  and concave backs while Asian elephants have smaller ears  and convex or level backs.\n"
     ]
    }
   ],
   "source": [
    "for index in top_sentence_indices:\n",
    "    print(sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用TextRank算法，我们最终得到了我们想要的摘要。他的内容也是非常有意义的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个通用函数以便在任何文档上计算基于TextRank的摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textrank_text_summarizer(documents,num_sentences = 2,feature_type = 'frequency'):\n",
    "    vec,dt_martix = build_feature_matrix(norm_sentences,feature_type='tfidf')\n",
    "    similarity_matrix = (dt_martix*dt_martix.T)\n",
    "    similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\n",
    "    scores = networkx.pagerank(similarity_graph)\n",
    "    ranked_sentences = sorted(((score,index)for index,score in scores.items()),\n",
    "                             reverse = True)\n",
    "    top_sentence_indices = [ranked_sentences[index][1] for index in range(num_sentences)]\n",
    "    top_sentence_indices.sort()\n",
    "    \n",
    "    for index in top_sentence_indices:\n",
    "        print(sentences[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成产品说明摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENT = \"\"\"\n",
    "The Elder Scrolls V: Skyrim is an open world action role-playing video game \n",
    "developed by Bethesda Game Studios and published by Bethesda Softworks. \n",
    "It is the fifth installment in The Elder Scrolls series, following \n",
    "The Elder Scrolls IV: Oblivion. Skyrim's main story revolves around \n",
    "the player character and their effort to defeat Alduin the World-Eater, \n",
    "a dragon who is prophesied to destroy the world. \n",
    "The game is set two hundred years after the events of Oblivion \n",
    "and takes place in the fictional province of Skyrim. The player completes quests \n",
    "and develops the character by improving skills. \n",
    "Skyrim continues the open world tradition of its predecessors by allowing the \n",
    "player to travel anywhere in the game world at any time, and to \n",
    "ignore or postpone the main storyline indefinitely. The player may freely roam \n",
    "over the land of Skyrim, which is an open world environment consisting \n",
    "of wilderness expanses, dungeons, cities, towns, fortresses and villages. \n",
    "Players may navigate the game world more quickly by riding horses, \n",
    "or by utilizing a fast-travel system which allows them to warp to previously \n",
    "Players have the option to develop their character. At the beginning of the game, \n",
    "players create their character by selecting one of several races, \n",
    "including humans, orcs, elves and anthropomorphic cat or lizard-like creatures, \n",
    "and then customizing their character's appearance.discovered locations. Over the \n",
    "course of the game, players improve their character's skills, which are numerical \n",
    "representations of their ability in certain areas. There are eighteen skills \n",
    "divided evenly among the three schools of combat, magic, and stealth. \n",
    "Skyrim is the first entry in The Elder Scrolls to include Dragons in the game's \n",
    "wilderness. Like other creatures, Dragons are generated randomly in the world \n",
    "and will engage in combat. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 13\n"
     ]
    }
   ],
   "source": [
    "sentences = parse_document(DOCUMENT)\n",
    "norm_sentences = normalize_corpus(sentences,lemmatize=True)\n",
    "print(\"Total Sentences:\",len(norm_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Elder Scrolls V: Skyrim is an open world action role-playing video game  developed by Bethesda Game Studios and published by Bethesda Softworks.\n",
      "Players may navigate the game world more quickly by riding horses,  or by utilizing a fast-travel system which allows them to warp to previously  Players have the option to develop their character.\n",
      "At the beginning of the game,  players create their character by selecting one of several races,  including humans, orcs, elves and anthropomorphic cat or lizard-like creatures,  and then customizing their character's appearance.discovered locations.\n"
     ]
    }
   ],
   "source": [
    "lsa_text_summarizer(norm_sentences,num_sentences=3,\n",
    "                   num_topics = 5,feature_type = 'frequency',\n",
    "                   sv_threshold = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Elder Scrolls V: Skyrim is an open world action role-playing video game  developed by Bethesda Game Studios and published by Bethesda Softworks.\n",
      "Players may navigate the game world more quickly by riding horses,  or by utilizing a fast-travel system which allows them to warp to previously  Players have the option to develop their character.\n",
      "Skyrim is the first entry in The Elder Scrolls to include Dragons in the game's  wilderness.\n"
     ]
    }
   ],
   "source": [
    "textrank_text_summarizer(norm_sentences,num_sentences=3,\n",
    "                        feature_type = 'tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
