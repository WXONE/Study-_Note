{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主题建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主题建模专门设计用于从包含各种类型文档的大型语料库中提取各种不同概念或主题，其中每个文档涉及一个或多个概念。这些概念可以是从思想到一件、事实、展望、陈述等。主题建模的主要目的是使用数学和统计技术来发现语料库中的隐藏和潜在语义结构。\n",
    "主题建模涉及从文档词项中提取特征，并使用矩阵分解和SVD等数学结构和框架来生成彼此不同的词簇或词组，并且这些词簇形成主题或概念。\n",
    "构建主题模型有各种框架和算法。我们将介绍以下三种方法：\n",
    "* 隐含语义索引\n",
    "* 隐含Dirichlet分布。\n",
    "* 非负矩阵分解\n",
    "我们将使用gensim和scikit-learn来进行实际的实现，并且还会介绍如何基于隐含语义索引来构建自己的主题模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from normalization3 import normalize_corpus\n",
    "import numpy as np\n",
    "\n",
    "toy_corpus = [\"The fox jumps over the dog\",\n",
    "\"The fox is very clever and quick\",\n",
    "\"The dog is slow and lazy\",\n",
    "\"The cat is smarter than the fox and the dog\",\n",
    "\"Python is an excellent programming language\",\n",
    "\"Java and Ruby are other programming languages\",\n",
    "\"Python and Java are very popular programming languages\",\n",
    "\"Python programs are smaller than Java programs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料库中共八个文档，前四个是关于动物的，后四个是关于编程语言的。一旦构建了一些主题建模框架，我们将使用相同的方式来生成原子亚马逊实际产品评论的主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含语义索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function normalize_corpus at 0x0000024DC3AA9C80>\n"
     ]
    }
   ],
   "source": [
    "norm_tokenized_corpus = normalize_corpus(toy_corpus,tokenize=True)\n",
    "print(normalize_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 0, 'fox': 1, 'jump': 2, 'clever': 3, 'quick': 4, 'lazy': 5, 'slow': 6, 'cat': 7, 'smarter': 8, 'excellent': 9, 'language': 10, 'programming': 11, 'python': 12, 'java': 13, 'ruby': 14, 'popular': 15, 'program': 16, 'small': 17}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "\n",
    "print (dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (1, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(10, 1), (11, 1), (13, 1), (14, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (13, 1), (15, 1)],\n",
       " [(12, 1), (13, 1), (16, 2), (17, 1)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在将对这个语料库建立一个TF-IDF加权模型，其中每个文档中的每个词将包含其TF-IDF权重。这类似于特征提取或向量空间转换，其中每个文档由其词的TF-IDF向量表示。完成之后我们将在这些特征上构建一个LSI模型，并输入我们想要生成的主题数量。这个数字是基于直觉和试错，所以在语料库上建立主题模型时，以可随意尝试这个参数。根据我们期望小语料库所包含的主题数量将此参数设置为2："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "total_topics = 2\n",
    "lsi = models.LsiModel(corpus_tfidf,\n",
    "                     id2word = dictionary,\n",
    "                     num_topics = total_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "-0.459*\"language\" + -0.459*\"programming\" + -0.344*\"python\" + -0.344*\"java\" + -0.336*\"popular\" + -0.318*\"excellent\" + -0.318*\"ruby\" + -0.148*\"program\" + -0.074*\"small\" + 0.000*\"quick\"\n",
      "Topic #2\n",
      "0.459*\"dog\" + 0.459*\"fox\" + 0.444*\"jump\" + 0.322*\"cat\" + 0.322*\"smarter\" + 0.208*\"quick\" + 0.208*\"clever\" + 0.208*\"slow\" + 0.208*\"lazy\" + 0.000*\"programming\"\n"
     ]
    }
   ],
   "source": [
    "for index,topic in lsi.print_topics(total_topics):\n",
    "    print('Topic #'+ str(index + 1))\n",
    "    print(topic)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数有助于在有阙值或无阙值的情况下以更好的方式显示主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics_gensim(topic_model,total_topics = 1,\n",
    "                       weight_threshold = 0.0001,\n",
    "                       display_weights = False,\n",
    "                       num_terms = None):\n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index)\n",
    "        topic = [(word,round(wt,2))for word,wt in topic \n",
    "                if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #' + str (index+1)+'with weights')\n",
    "            print(topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print('Topic #' + str(index+1)+'without weights')\n",
    "            tw = [term for term,wt in topic]\n",
    "            print(tw[:num_terms])if num_terms else tw\n",
    "        print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用以下代码对小语料库的主题模型测试这个函数，以了解如何获取主题并调整参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1without weights\n",
      "['language', 'programming', 'python', 'java', 'popular']\n",
      "Topic #2without weights\n",
      "['dog', 'fox', 'jump', 'cat', 'smarter']\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(topic_model=lsi,\n",
    "                   total_topics = total_topics,\n",
    "                   num_terms = 5,\n",
    "                   display_weights = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1with weights\n",
      "[('language', -0.46), ('programming', -0.46), ('python', -0.34), ('java', -0.34), ('popular', -0.34)]\n",
      "Topic #2with weights\n",
      "[('dog', 0.46), ('fox', 0.46), ('jump', 0.44), ('cat', 0.32), ('smarter', 0.32)]\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(topic_model=lsi,\n",
    "                   total_topics = total_topics,\n",
    "                   num_terms = 5,\n",
    "                   display_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经成功的使用LSI构建了一个主题建模框架，它可以从文档语料库中区分和显示主题。\n",
    "现在我们使用SVD从头开始构建自己的LSI主题模型框架。我们首先建立一个TF-IDF特征矩阵，实际上是一个文档-词项矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils_in_NLP import build_feature_matrix,low_rank_svd\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type = 'tfidf')\n",
    "td_matrix = tfidf_matrix.transpose()\n",
    "td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "total_topics = 2\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成后使用low_rank_svd()函数计算我们的词项-文档矩阵的SVD，以便我们构建一个只取前k个奇异向量的低秩矩阵逼近，这将等于我们在此情况下的主题数量。通过是用S和U分量，我们将他们一起相乘以生成每个主题的每个词频及其权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u , s , vt = low_rank_svd(td_matrix,singular_count=total_topics)\n",
    "weights = u.transpose() * s[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了词项的权重，需要将他们连接回到我们的词项。我们定义两个效用函数，用于通过连接词项与权重来生成这些主题，然后使用具有可配置参数的函数来打印这些主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1]) \n",
    "                           for row \n",
    "                           in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index]) \n",
    "                               for wt, index \n",
    "                               in zip(weights,sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row]) \n",
    "                             for row \n",
    "                             in sorted_indices])\n",
    "    \n",
    "    topics = [np.vstack((terms.T, \n",
    "                     term_weights.T)).T \n",
    "              for terms, term_weights \n",
    "              in zip(sorted_terms, sorted_weights)]     \n",
    "    \n",
    "    return topics            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        topic = [(word, round(wt,2)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "                     \n",
    "        if display_weights:\n",
    "            print ('Topic #'+str(index+1)+' with weights')\n",
    "            print (topic[:num_terms]) if num_terms else topic\n",
    "        else:\n",
    "            print ('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print( tw[:num_terms]) if num_terms else tw\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics = topics,\n",
    "                total_topics = total_topics,\n",
    "                weight_threshold = 0,\n",
    "                display_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(\n",
    "                topics = topics,\n",
    "                total_topics = total_topics,\n",
    "                weight_threshold = 0.15,\n",
    "                display_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用LSI来定义下面的函数作为通用可重用的主题建模框架："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lsi_model_gensim(corpus,total_topics = 2):\n",
    "    norm_tokenized_corpus = normalize_corpus(corpus,tokenize = True)\n",
    "    dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "    mapped_corpus = [dictionary.doc2bow(text)\n",
    "                    for text in norm_tokenized_corpus]\n",
    "    tfidf = models.TfidfModel(mapped_corpus)\n",
    "    corpus_tfidf = tfidf[mapped_corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf,\n",
    "                         id2word = dictionary,\n",
    "                         num_topics = total_topics)\n",
    "    return lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含Dirichlet(狄利克利)分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "隐含Dirichlet分布技术是一种概率生成模型，其中假定每个文档具有类似于概率隐含语义索引模型的主题组合--但是在此情况下，隐含主题包含他们的Dirichlet先验分布。这项技术背后的数学知识比较复杂，因为他的具体细节将超出当前范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "假设我们有M个文档，N个文档中的单词，以及K个想要生成的主题数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法思想：\n",
    "1 初始化必要参数。\n",
    "2 对于每个文档，随机将每个单词初始化为K个主题之一。\n",
    "3 开始如下的一个迭代过程，重复几次。\n",
    "4 对于每个文档D：\n",
    "a 对于文档中的每个单词W：\n",
    "* 对于每个主题T\n",
    "* 计算P(T|D),其是D中分配给主题T的词的比例\n",
    "* 计算P(W|D),其是对于含有词W的所有文档分配给主题T的比例。\n",
    "* 考虑所有其他单词及其主题分配，用主题T和概率P(T|D)*P(W|D)重新分配词W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行了几次迭代之后，我们应该为每个文档提供主题混合，然后从指向该主题的词中生成每个主题的组成部分。我们在以下实现中使用gensim来构建基于LDA的主题模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_lda_model_gensim(corpus,total_topics = 2):\n",
    "    norm_tokenized_corpus = normalize_corpus(corpus,tokenize = True)\n",
    "    dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "    mapped_corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n",
    "    tfidf = models.TfidfModel(mapped_corpus)\n",
    "    corpus_tfidf = tfidf[mapped_corpus]\n",
    "    lda = models.LdaModel(corpus_tfidf,\n",
    "                         id2word = dictionary,\n",
    "                         iterations = 1000,\n",
    "                         num_topics = total_topics)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1with weights\n",
      "[('programming', 0.07), ('java', 0.07), ('ruby', 0.07), ('language', 0.07), ('program', 0.06)]\n",
      "Topic #2with weights\n",
      "[('excellent', 0.07), ('fox', 0.07), ('python', 0.07), ('dog', 0.06), ('clever', 0.06)]\n"
     ]
    }
   ],
   "source": [
    "lda_gensim = train_lda_model_gensim(toy_corpus,\n",
    "                                   total_topics = 2)\n",
    "print_topics_gensim(topic_model = lda_gensim,\n",
    "                   total_topics = 2,\n",
    "                   num_terms = 5,\n",
    "                   display_weights  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type='tfidf')\n",
    "total_topics = 2\n",
    "lda = LatentDirichletAllocation(n_topics = total_topics,\n",
    "                               max_iter = 100,\n",
    "                               learning_method = 'online',\n",
    "                               learning_offset = 50,\n",
    "                               random_state = 42)\n",
    "lda.fit(tfidf_matrix)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = lda.components_\n",
    "topics = get_topics_terms_weights(weights,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该段代码中，将LDA模型应用于文档-词项的TF-IDF特征矩阵，其被分解成两个矩阵，即一个文档-主题矩阵和一个主题-词项矩阵。我们使用存储在lda.components_ 中的主题-词项来检索每个主题每个词的权重。得到这些权重后，我们使用LSI建模中的get_topics_terms_weights()函数根据每个主题的词项和权重来构建主题。我们现在可以使用之前实现的print_topics_udf()函数查看主题："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('fox', 1.85), ('dog', 1.54), ('jump', 1.17), ('clever', 1.11), ('quick', 1.11), ('cat', 1.06), ('smarter', 1.05), ('excellent', 0.6)]\n",
      "Topic #2 with weights\n",
      "[('programming', 1.73), ('language', 1.73), ('java', 1.61), ('python', 1.58), ('program', 1.29), ('ruby', 1.09), ('slow', 1.08), ('lazy', 1.08)]\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics = topics,\n",
    "                 total_topics = total_topics,\n",
    "                num_terms = 8,\n",
    "                display_weights = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非负矩阵分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非负矩阵分解(NNMF)是一种类似于SVD的矩阵分解技术，虽然NNMF是对非负矩阵操作运算，并也可适用于多变量数据。NNMF可定义为：给定非负矩阵V，目标是找到两个非负矩阵因子W和H，使得他们相乘时，他们可以近似重构V，数学上这表示为V≈W * H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使得所有三个矩阵都为非负。为了实现这个近似，我们通常使用一个成本函数，如两个矩阵之间的欧几里得距离是L2范数，或是L2范数略微修改的Frobenius范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从scikit-learn decomposition模块的NMF类中可获得该实现。\n",
    "可以在我们的小语料库上使用以下代码构建一个基于NNMF的主题模型，他给出了与LDA一样的特征名称和权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer,tfidf_matrix = build_feature_matrix(norm_corpus,feature_type='tfidf')\n",
    "total_topics = 2\n",
    "nmf = NMF(n_components = total_topics,\n",
    "         random_state = 42,alpha = .1,l1_ratio = .5)\n",
    "nmf.fit(tfidf_matrix)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = nmf.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了词项及其权重，可以使用我们以前定义的函数来打印主题，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "Topic #2 with weights\n"
     ]
    }
   ],
   "source": [
    "topics = get_topics_terms_weights(weights,feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                total_topics = total_topics,\n",
    "                num_terms = None,\n",
    "                display_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从产品评论中提取主题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将获取上古卷轴的在亚马逊上的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I base the value of a game on the amount of enjoyable gameplay I can get out of it and this one was definitely worth the price!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CORPUS = pd.read_csv('amazon_skyrim_reviews.csv')\n",
    "CORPUS = np.array(CORPUS['Reviews'])\n",
    "print(CORPUS[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I base the value of a game on the amount of enjoyable gameplay I can get out of it and this one was definitely worth the price!\n",
      "Topic #1without weights\n",
      "['much', 'fun', 'like', 'skyrim', 'love', 'time', 'quest', 'level', 'good', 'one']\n",
      "Topic #2without weights\n",
      "['love', 'skyrim', 'dragon', 'play', 'would', 'like', 'great', 'buy', 'series', 'oblivion']\n",
      "Topic #3without weights\n",
      "['get', 'one', 'play', 'recommend', 'quest', 'hour', 'fun', 'great', 'want', 'review']\n",
      "Topic #4without weights\n",
      "['buy', 'main', 'play', 'good', 'come', 'scroll', 'still', 'oblivion', 'skyrim', 'well']\n",
      "Topic #5without weights\n",
      "['play', 'love', 'best', 'get', 'ever', 'one', 'hour', 'fun', 'go', 'son']\n",
      "Topic #1without weights\n",
      "['great', 'get', 'good', 'buy', 'one', 'play', 'best', 'oblivion', 'elder', 'scroll']\n",
      "Topic #2without weights\n",
      "['like', 'one', 'skyrim', 'level', 'dragon', 'get', 'play', 'quest', 'even', 'make']\n",
      "Topic #3without weights\n",
      "['play', 'many', 'give', 'love', 'time', 'good', 'go', 'think', '5', 'find']\n",
      "Topic #4without weights\n",
      "['fun', 'love', 'start', 'good', 'really', 'recommend', 'great', 'huge', 'play', 'review']\n",
      "Topic #5without weights\n",
      "['skyrim', 'best', 'play', 'ever', 'much', 'quest', 'oblivion', 'love', 'like', 'come']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 without weights\n",
      "['estatic', 'booklet', 'wonder4ful', 'electricity', 'heat', 'trhats', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #2 without weights\n",
      "['game', 'play', 'get', 'one', 'skyrim', 'great', 'like', 'time', 'quest', 'much']\n",
      "Topic #3 without weights\n",
      "['de', 'crédito', 'pagar', 'momento', 'compras', 'responsabilidad', 'para', 'recomiendo', 'futuras', 'skyrimseguridad']\n",
      "Topic #4 without weights\n",
      "['booklet', 'estatic', 'wonder4ful', 'electricity', 'heat', 'trhats', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #5 without weights\n",
      "['estatic', 'booklet', 'wonder4ful', 'electricity', 'trhats', 'heat', 'amazingly', 'interfere', 'chirstmas', '12yr']\n",
      "Topic #1 without weights\n",
      "['game', 'get', 'skyrim', 'play', 'time', 'quest', 'like', 'one', 'go', 'much']\n",
      "Topic #2 without weights\n",
      "['game', 'recommend', 'love', 'great', 'highly', 'play', 'wonderful', 'like', 'would', 'graphic']\n",
      "Topic #3 without weights\n",
      "['scroll', 'elder', 'series', 'always', 'love', 'pass', 'franchise', 'buy', 'game', 'far']\n",
      "Topic #4 without weights\n",
      "['ever', 'best', 'game', 'play', 'rpg', 'one', 'ive', 'hour', 'great', 'definitely']\n",
      "Topic #5 without weights\n",
      "['fun', 'game', 'much', 'graphic', 'improvement', 'mission', 'expect', 'see', 'hour', 'couple']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "                 \n",
    "CORPUS = pd.read_csv('amazon_skyrim_reviews.csv')                     \n",
    "CORPUS = np.array(CORPUS['Reviews'])\n",
    "\n",
    "# view sample review\n",
    "print(CORPUS[12])\n",
    "\n",
    "        \n",
    "total_topics = 5\n",
    "        \n",
    "lsi_gensim = train_lda_model_gensim(CORPUS,\n",
    "                                    total_topics=total_topics)\n",
    "print_topics_gensim(topic_model=lsi_gensim,\n",
    "                    total_topics=total_topics,\n",
    "                    num_terms=10,\n",
    "                    display_weights=False) \n",
    "\n",
    "lda_gensim = train_lda_model_gensim(CORPUS,\n",
    "                                    total_topics=total_topics)\n",
    "print_topics_gensim(topic_model=lda_gensim,\n",
    "                    total_topics=total_topics,\n",
    "                    num_terms=10,\n",
    "                    display_weights=False) \n",
    "\n",
    "\n",
    "norm_corpus = normalize_corpus(CORPUS)\n",
    "vectorizer, tfidf_matrix = build_feature_matrix(norm_corpus, \n",
    "                                    feature_type='tfidf') \n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=total_topics, \n",
    "                                max_iter=1000,\n",
    "                                learning_method='online', \n",
    "                                learning_offset=10.,\n",
    "                                random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "weights = lda.components_\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 num_terms=10,\n",
    "                 display_weights=False)\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=total_topics, \n",
    "          random_state=42, alpha=.1, l1_ratio=.5)\n",
    "nmf.fit(tfidf_matrix)      \n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = nmf.components_\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 num_terms=10,\n",
    "                 display_weights=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 自动文档摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动化文档摘要的主要目标是不包括人工输入的执行此摘要，除了运行任何计算机程序。数学和统计模型有助于通过观察其内容和上下文来构建和自动化概况文档的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用自动化技术进行文档摘要主要有两大类做法：\n",
    "* 基于提取的技术：\n",
    "这些方法使用数学和统计学概念（如SVD）从原始文档中提取内容的一些关键子集，使得该内容子集包含核心信息，并作为整个文档的重点。这个内容可以是单词、短语或句子。这种方法的最终结果是从原始文档中采集或提取了几行简短的执行摘要。在这种技术中不产生新的内容--因此这个名称是基于提取的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基于概括的技术：这些方法更加复杂和精准，并利用语言语义来产生表示。他们还利用NLG技术，其中及其使用知识库和语义表达来自己生成文本，并像人类编写一样来创建摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过利用gensim的摘要模块来看看文档摘要的实现。我们将使用维基百科关于大象的描述来作为我们将测试所有摘要技术的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization3 import normalize_corpus,parse_document\n",
    "from utils_in_NLP import build_feature_matrix,low_rank_svd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在定义一个函数将输入文档总结道其原始大小的一小部分，这将作为下面函数中的用户输入参数summary_ratio.输出将是摘要后的文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize,keywords\n",
    "def text_summarization_gensim(text,summary_ratio = 0.5):\n",
    "    summary = summarize(text,split = True,ratio = summary_ratio)\n",
    "    for sentence in summary:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two species are traditionally recognised,  the African elephant and the Asian elephant.\n",
      "All  elephants have a long trunk used for many purposes,  particularly breathing, lifting water and grasping objects.\n",
      "Elephants' large ear flaps help  to control their body temperature.\n"
     ]
    }
   ],
   "source": [
    "docs = parse_document(toy_text)\n",
    "text = ' '.join(docs)\n",
    "text_summarization_gensim(text,summary_ratio=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文档共有9个句子，观察后发现，总结后共有三个句子，但是文档的核心意义与主题已被保留。\n",
    "这个原子gensim的摘要实现是基于一种流行的称为TextRank的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将主要关注以下技术：\n",
    "* 隐含语义分析\n",
    "* TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#解析和规范化文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences in Documents: 9\n"
     ]
    }
   ],
   "source": [
    "sentences = parse_document(toy_text)\n",
    "norm_sentences = normalize_corpus(sentences,lemmatize=True)\n",
    "total_sentences = len(norm_sentences)\n",
    "print('Total Sentences in Documents:',total_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦有了一个可运用得摘要算法，我们将为每种技术构建一个通用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隐含语义分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐含语义分析（LSA）的核心原则是，在任何文件中，在词语的相关语境中存在隐含的结构，因此也应该在相同的奇异空间中相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
