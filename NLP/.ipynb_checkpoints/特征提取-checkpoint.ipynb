{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几种特征提取技术：\n",
    "* 词袋模型\n",
    "* TF-IDF模型\n",
    "* 高级词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词袋模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型也许是从文本文档中提取特征最简单但又最有效的技术。这个模型的本质时将文本文档转化为向量，从而将每个文档转化为一个向量，这个向量表示在文档空间中全部不同的单词在该文档中出现的频率。因此，根据前面的数学定义，这里的例子向量记为D，每个单词的权重与该词在文档中出现的频率相等。\n",
    "可以为单个单词出现频率和n元分词出现频率建立同样的模型，该模型就是n元分词词袋模型，他计算不同的n元分词在每个文档中出现的频率。\n",
    "下面的代码片段给出了一个函数，实现了基于词袋的特征提取模块，该模块也接受ngram_range参数作为n元分词的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bow_extractor(corpus,ngram_range = (1,3)):\n",
    "    vectorizer = CountVectorizer(min_df = 1,ngram_range = ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer,features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的函数使用CountVectorizer类，可以设置ngram_range为不同的参数值，如(1,3)，将建立包括所有unigram、bigram和trigram的向量空间。下面的代码片段显示函数在样本语料，即4个训练文档和1个测试文档中的执行情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 2 0 2 0 0]\n",
      " [0 1 1 0 1 0 1 1 1]\n",
      " [0 0 1 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "bow_vectorizer,bow_features = bow_extractor(CORPUS)\n",
    "features = bow_features.todense()\n",
    "print (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "print(new_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'beautiful', 'blue', 'cheese', 'is', 'love', 'sky', 'so', 'the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述输出显示出每个文档如何转换为向量，每行代表语料库中的一个文档。我们对两个语料库均执行相同操作。使用CORPUS变量中的文档建立了向量生成器。用它来提取特征，使用这个建立的向量生成器从全新的文档中提取特征。响亮的每一列描述的单词在feature_names变量中描述，每列的值是该词在文档中的频率，第一次看到他时可能难于理解，因此我们准备了以下函数，它有助于更好的理解特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def display_features(features,feature_names):\n",
    "    df = pd.DataFrame(data = features,columns = feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(features,feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "display_features(new_doc_features,feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词袋模型的向量依赖于单词出现的绝对频率。这存在一些潜在问题，语料库全部文档中出现次数较多的单词将会拥有较高的频率，这些词将会影响其他一些出现不如这些词频繁但对于文档分类更有意义和有效的单词。这就是TF-IDF的来源。TF-IDF代表的是词频—逆文档频率，是两个度量的组合。\n",
    "数学上。TF-IDF是两个度量的乘机，可以表示为tfidf = tf*idf。\n",
    "词频tf，由词袋模型计算得出。\n",
    "逆文档词频由idf表示，是每个单词的文档频率的逆，该值由语料库中全部文档数量除以每个单词的文档频率，然后对结果应用对数运算变换其比例。在这里的实现中将对每个单词的文档频率+1，意味着词汇表中每个单词至少包含一个在语料库文档中，这是为了避免被0除的错误，平滑逆文档频率。也对idf的计算结果+1，避免被忽略单词拥有0值的idf。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
