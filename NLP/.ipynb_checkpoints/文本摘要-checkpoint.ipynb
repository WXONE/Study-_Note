{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设有一整套文本文档语料库，其范围从句子到段落，你的任务是尝试从中获得有意义的见解。以下是可以对文本文档进行的一些操作：\n",
    "* 提取文档中关键影响短语\n",
    "* 提取文档中存在的各种不同的概念或主题\n",
    "* 总结文件，以提供保留着整个语料库重要部分的要点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将讨论三种主要的概念和技术：\n",
    "* 关键短语提取\n",
    "* 主题模型\n",
    "* 自动文摘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个重要概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档通常是一个包含完整文本数据的实体，包含可选的标题和其他元数据信息。语料库通常由一系列文档组成。这些文档可以是简单的句子或完整的文本信息段落。分词语料库指的是每个文档被分词化或分解成标识的语料库，其中标识通常是单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我呢本规范化是通过技术来清洗、规范化和标准化文本数据的过程，譬如删除特殊符号和字符、去除多余的HTML标签、移除停用词、校正拼写(中文应该是修改错别字)、词干提取和词形还原"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征提取是我们从原始文本数据中提取有意义的特征或属性，以将其提供给统计或机器学习算法的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征矩阵通常是指从文档集合到特征的映射，其中每行表示文档，每列表示具体特征，特征通常是一个单词或一组单词。我们将通过特征提取后的特征矩阵来表达文档或句子的集合，并且将在后面的实例中经常在统计和机器学习技术之中应用这些矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 奇异值分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值分解（SVD）是线性代数的一种技术，他在摘要法中经常使用。SVD是实数或复数矩阵的因子分解过程。我们将使用源自scipy的一个很好的实现来提取顶部的k个奇异值，并返回相应的U、S和V矩阵。在utils.py文件中我们将使用以下代码段："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "def low_rank_svd(matrix,singular_count = 2):\n",
    "    u,s,vt = svds(matrix,k = singular_count)\n",
    "    return u,s,vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将在主题建模以及有关文本摘要的中使用该函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该过程的主要步骤包括\n",
    "* 句子提取\n",
    "* 取消HTML转移序列\n",
    "* 扩展缩写词\n",
    "* 文本还原\n",
    "* 删除特殊字符\n",
    "* 删除停用词\n",
    "在第一步中我们将在收到的文本文档中，执行：删除其换行符，解析文本，将其转换为ASCII格式，并将其分解成其句子成分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_document(document):\n",
    "    document = re.sub('\\n',' ',document)\n",
    "    if isinstance(document,str):\n",
    "        document = document\n",
    "    elif isinstance(document,unicode):\n",
    "        return unicodedata.normalize('NFKD',document).encode('ascii','ignore')\n",
    "    else:\n",
    "        raise ValueError('Document is not string or unicode!')\n",
    "    document = document.strip()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步骤二涉及被转义或编码的非转义特殊HTML字符。我们使用以下函数来取消他们的转义，并将它们恢复到原来未转义形式，这样便可在后续阶段规范化他们：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "html_parser = HTMLParser()\n",
    "def unescape_html(parser,text):\n",
    "    return parser.unescape(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们要在最终规范化函数中用参数表示词形还原操作，以使其可选，因为在某些情况下我们不想使用词形还原。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,lemmatize = True ,tokenize = False):\n",
    "    normalize_corpus = []\n",
    "    for text in corpus:\n",
    "        text = html_parser.unescape(text)\n",
    "        text = expand_contractions(text,CONTRACTION_MAP)\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text)\n",
    "        else:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalize_corpus.append(text)\n",
    "        else:\n",
    "            normalize_corpus.append(text)\n",
    "            \n",
    "    return normalize_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用该函数来满足大部分的规范化需要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用泛型函数从文本数据中执行各种类型的特征提取。将要使用的特征类型如下：\n",
    "* 基于词项次数的二值特征\n",
    "* 基于词袋模型的频率特征\n",
    "* TF-IDF权重模型\n",
    "\n",
    "我们将使用下面的函数实现从文本文档中提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "def build_feature_matrix(documents,feature_type = 'frequency'):\n",
    "    feature_type = feature_type.lower().strip()\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary = True,min_df = 1,ngram_range = (1,1))\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary = False,min_df = 1,ngram_range = (1,1))\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df = 1,ngram_range = (1,1))\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered.Possible values:'binary','frequence','tfidf'\")\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    return vectorizer,feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键短语提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从非结构化文本文档中提取重要信息的最简单但最强大的技术之一是关键短语提取，关键短语提取也称为词项提取，定义为非结构化文本提取关键重要和相关词项或短语的过程或技术，使得文本文档的核心论题或主题涵盖在这些关键短语中。这种技术属于信息检索和提取的广泛领域。关键短语提取可以应用在许多领域，包括：\n",
    "* 语义网\n",
    "* 基于查询的搜索引擎和爬虫\n",
    "* 推荐系统\n",
    "* 标注系统\n",
    "* 文档相似性\n",
    "* 翻译\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有各种各样的关键短语提取方法，下面介绍两种：\n",
    "* 搭配\n",
    "* 基于权重标签的短语提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有各种方法提取搭配，最好方法之一是使用一种n元分词分组或分割方法，我们从语料库中构造n元分词分组，计算每个n元分词的频率，并根据他们的出现频率进行排序以得到最频繁的n元分词搭配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用古滕堡语料库的书的一部分内容作为我们的语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice adventures wonderland lewis carroll 1865\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from normalization3 import normalize_corpus\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "#下载语料\n",
    "alice = gutenberg.sents(fileids='carroll-alice.txt')\n",
    "alice = [' '.join(ts) for ts in alice]\n",
    "#在python3里filter不再返回list，需要强转\n",
    "norm_alice =list(filter(None, normalize_corpus(alice, lemmatize=False)))\n",
    "#打印第一行\n",
    "print(norm_alice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence,n):\n",
    "    return zip(*[sequence[index:] for index in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x0000017C26B07D48>\n"
     ]
    }
   ],
   "source": [
    "print(compute_ngrams([1,2,3,4],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码帮助我们获得最大的n元分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_ngrams(corpus,ngram_val = 1,limit = 5):\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    \n",
    "    ngrams = compute_ngrams(tokens,ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),key = itemgetter(1),reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[:limit]\n",
    "    sorted_ngrams = [(' '.join(text),freq) for text,freq in sorted_ngrams]\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus = norm_alice,ngram_val=2,limit = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_ngrams(corpus=norm_alice,ngram_val=3,limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "finder = BigramCollocationFinder.from_documents([\n",
    "    item.split()\n",
    "    for item\n",
    "    in norm_alice\n",
    "])\n",
    "bigram_measures = BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.raw_freq,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abide', 'figures'),\n",
       " ('acceptance', 'elegant'),\n",
       " ('accounting', 'tastes'),\n",
       " ('accustomed', 'usurpation'),\n",
       " ('act', 'crawling'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trusts'),\n",
       " ('agony', 'terror'),\n",
       " ('alarmed', 'proposal')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "finder = TrigramCollocationFinder.from_documents([\n",
    "    item.split()\n",
    "    for item\n",
    "    in norm_alice\n",
    "])\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "finder.nbest(trigram_measures.raw_freq,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(trigram_measures.pmi,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于权重标签的短语提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "这是一个稍微不同的方法，算法遵循以下步骤：\n",
    "* 使用浅层分析提取所有的名词短语词块\n",
    "* 计算每个词块的TF-IDF权重并返回最大加权短语\n",
    "使用从维基百科获取的关于大象的示例描述："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_text = \"\"\"\n",
    "Elephants are large mammals of the family Elephantidae \n",
    "and the order Proboscidea. Two species are traditionally recognised, \n",
    "the African elephant and the Asian elephant. Elephants are scattered \n",
    "throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male \n",
    "African elephants are the largest extant terrestrial animals. All \n",
    "elephants have a long trunk used for many purposes, \n",
    "particularly breathing, lifting water and grasping objects. Their \n",
    "incisors grow into tusks, which can serve as weapons and as tools \n",
    "for moving objects and digging. Elephants' large ear flaps help \n",
    "to control their body temperature. Their pillar-like legs can \n",
    "carry their great weight. African elephants have larger ears \n",
    "and concave backs while Asian elephants have smaller ears \n",
    "and convex or level backs.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好了语料库，我们将使用模式“NP:{<DT>? <JJ>*<NN.*>+}\"来从文档/句子的语料库中提取所有可能的名词短语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization3 import parse_document\n",
    "import itertools\n",
    "import nltk\n",
    "from normalization3 import stopword_list\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "def get_chunks(sentences, grammar = r'NP:{<DT>? <JJ>* <NN.*>+}'):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tagged_sents = nltk.pos_tag_sents(\n",
    "                            [nltk.word_tokenize(sentence)])\n",
    "        \n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                  for tagged_sent in tagged_sents]\n",
    "        \n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                     for chunk in chunks]    \n",
    "         \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        \n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                        for status, chunk \n",
    "                        in itertools.groupby(flattened_chunks,\n",
    "                                             lambda word,pos,chunk: \n",
    "                                             chunk != 'O')]\n",
    "        \n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk \n",
    "                                in wtc_group \n",
    "                                    if word.lower() \n",
    "                                        not in stopword_list) \n",
    "                                    for status, wtc_group \n",
    "                                    in valid_chunks_tagged\n",
    "                                        if status]\n",
    "                                            \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 基本上我们有一个已定义的语法模式来分块或提取名词短语。我们在同意模式中定义一个分块器，对于文档中的每个句子，首先用他的POS标签来标注他（因此不应该对文本进行规范化），然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于pos标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conlltags函数来生成（w,t,c）三元组。删除所有带有‘0’标签的标签，因为他们基本上不属于任何词块的单词或词项。最后从有效词块中组合分块的词项，并从每个词块分组中生成短语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 2 required positional arguments: 'pos' and 'chunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-cb1393651d3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoy_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvalid_chunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-f3e07d386353>\u001b[0m in \u001b[0;36mget_chunks\u001b[1;34m(sentences, grammar)\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         in itertools.groupby(flattened_chunks,\n\u001b[1;32m---> 32\u001b[1;33m                                              \u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                                              chunk != 'O')]\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-f3e07d386353>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m                            )\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n\u001b[0m\u001b[0;32m     30\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                         in itertools.groupby(flattened_chunks,\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() missing 2 required positional arguments: 'pos' and 'chunk'"
     ]
    }
   ],
   "source": [
    "sentences = parse_document(toy_text)          \n",
    "valid_chunks = get_chunks(sentences)\n",
    "print (valid_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
