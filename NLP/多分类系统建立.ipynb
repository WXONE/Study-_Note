{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   我们将文本规范化、特征提取、建模、评估结合在一起，建立一个多分类文本分类系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset = 'all',\n",
    "                             shuffle = True,\n",
    "                             remove = ('headers','footers','quotes'))\n",
    "    return data\n",
    "def prepare_datasets(corpus,labels,test_data_proportion = 0.3):\n",
    "    train_X,test_X,train_Y,test_Y = train_test_split(corpus,labels,test_size = 0.33,\n",
    "                                                    random_state = 42)\n",
    "    return train_X,test_X,train_Y,test_Y\n",
    "def remove_empty_docs(corpus,labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc,label in zip(corpus,labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "            \n",
    "    return filtered_corpus,filtered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经获取了数据，查看了数据集中分类的数量，使用下面的代码将数据集分为测试数据集和训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "dataset = get_data()\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus,labels = dataset.data,dataset.target\n",
    "corpus,labels = remove_empty_docs(corpus,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samle document: the blood of the lamb.\n",
      "\n",
      "This will be a hard task, because most cultures used most animals\n",
      "for blood sacrifices. It has to be something related to our current\n",
      "post-modernism state. Hmm, what about used computers?\n",
      "\n",
      "Cheers,\n",
      "Kent\n",
      "Class label : 19\n",
      "Actual class label: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "print('Samle document:',corpus[10])\n",
    "print('Class label :',labels[10])\n",
    "print('Actual class label:',dataset.target_names[labels[10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = prepare_datasets(corpus,labels,test_data_proportion  = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normalization2 import normalize_corpus\n",
    "norm_train_corpus = normalize_corpus(train_corpus)\n",
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-08a6c591e77b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                               \u001b[0mmin_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                               sample = 1e-3)\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mavg_wv_train_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_word_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mavg_wv_test_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maveraged_word_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Study-_Note\\NLP\\feature_extractors.py\u001b[0m in \u001b[0;36maveraged_word_vectorizer\u001b[1;34m(corpus, model, num_features)\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "from feature_extractors import bow_extractor,tfidf_extractor\n",
    "from feature_extractors import averaged_word_vectorizer\n",
    "from feature_extractors import tfidf_weighted_averaged_word_vectorizer\n",
    "import nltk\n",
    "import gensim\n",
    "bow_vectorizer,bow_train_features = bow_extractor(norm_train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n",
    "\n",
    "tfidf_vectorizer,tfidf_train_features = tfidf_extractor(norm_train_corpus)\n",
    "tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n",
    "tokenized_train = [nltk.word_tokenize(text) for text in norm_train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text) for text in norm_test_corpus]\n",
    "model = gensim.models.Word2Vec(tokenized_train,\n",
    "                              size = 500,\n",
    "                              window = 100,\n",
    "                              min_count = 30,\n",
    "                              sample = 1e-3)\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus = tokenized_train,model = model,num_features = 500)\n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus = tokenized_test,model = model,num_features = 500 )\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf_wv_train_features = tfidf_weighted_averaged_word_vectorizer(corpus = tokenized_train,\n",
    "                                       tfidf_vectorizer = tfidf_train_features,\n",
    "                                       tfidf_vocabulary = vocab,model = model,\n",
    "                                       num_features = 500)\n",
    "tfidf_wv_test_features = tfidf_weighted_averaged_word_vectorizer(corpus = tokenized_test,\n",
    "                                       tfidf_vectorizer = tfidf_test_features,\n",
    "                                       tfidf_vocabulary = vocab,model = model,\n",
    "                                       num_features = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用上面的特征提取器从文本文档中提取了全部必要的特征后，基于前面讨论的四个指标，我们定义一个函数用来评估分类模型，函数如下面的代码段所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "def get_metrics(trur_labels,predicted_labels):\n",
    "    print('Accuracy:',np.round(metrics.accuracy_score(true_labels,predicted_labels),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
