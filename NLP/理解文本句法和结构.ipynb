{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS标签器推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在这里讨论一些标记句子的推荐方法，第一种方法是使用推荐的ｐｏｓ＿ｔａｇ（）函数，它基于Penn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens,tagset = 'universal')\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以使用pattern模块通过以下代码段获取句子的pos标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sent = tag(sentence)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立自己的POS标签器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将讨论一些构建自己的POS标签器的技术，并利用nltk提供的一些类来实现他们。为了评估我们的标签器的性能，我们会使用nltk中treebank语料库的一些测试数据。我们还将使用一些训练数据来训练标签器。首先通过读取已标记的treebank语料库，我们可以获取训练和评估标签器的必要数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用测试数据来评估标签器，并使用例句的标识作为输入来验证标签器的工作效果，我们在nltk中使用的所有标签器均来自nltk.tag包。每个标签器都是基类Tagger1类的子类，并且每个标签器都执行一个tag()函数，它将一个句子的标识列表作为输入，返回带有POS标签的相同单词列表作为输出。除了标记外，还有一个evaluate()函数用于评估标签器的性能。它通过标记每个输入测试语句，然后将输出结果与句子的实际标签进行对比来完成评估。我们将使用该函数来测试我们的标签器在test_data上的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们看看从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "print(dt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的输出可以看出，在树库(treebank)测试数据集中我们已经获得了14%的单词正确标记率，并不是很理想，现在我们将使用正则表达式和RegexpTagger来尝试构建一个性能更好的标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21425113757382128\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "patterns = [\n",
    "    (r'.*ing$','VBG'),      #动名词\n",
    "    (r'.*ed$','VBD'),       #一般过去式\n",
    "    (r'.*es$','VBZ'),       #3rd singular present\n",
    "    (r'.*ould$','MD'),      #modals\n",
    "    (r'.*\\'s$','NN$'),      #possessive nouns\n",
    "    (r'.*s$','NNS'),        #plural nouns\n",
    "    (r'^->[0-9]+(.[0-9]+)?$','CD'),#cardinal numbers\n",
    "    (r'.*','NN')            #nouns (default)...\n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "print (rt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在将训练一些n元分词标签器，n元分词是来自文本序列或者语言序列的n个连续项。这些项可以由单词、音素、字母、字符或音节组成。Shingles是只包含单词的n元分词，我们将使用大小为1，2，3的n元分词，他们分别成为一元分词，二元分词和三元分词。UnigramTagger、BigramTagger和TrigramTagger继承自基类NGramTagger，NGramTagger类则继承自ContextTagger类，该类又继承自SequentialBackoffTagger类。我们将使用train_data作为训练数据，根据与举报标识及其POS标签来训练n元分词标签器，然后我们将在test_data上评估训练后的标签器，并查看例句的标记结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607803272340013\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print(ut.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(ut.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13466937748087907\n"
     ]
    }
   ],
   "source": [
    "print(bt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(bt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08064672281924679\n"
     ]
    }
   ],
   "source": [
    "print(tt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(tt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的数据清楚的表明，我们仅使用UnigramTagger标签器就可以在测试集上获得86%的准确率，这个结果与我们前一个标签器相比要好得多，二元分词与三元分词模型的准确性远不及一元分词模型，因为在训练数据中观察到的二元词组与三元词组不一定会在测试数据中以相同的方式出现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在通过创建一个包含标签列表的组合标签器以及使用backoff标签器，我们将尝试组合运用所有的标签器。本质上我们将创建一个标签器链，对于每一个标签器，如果他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_tagged(train_data,taggers,backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data,backoff = backoff)\n",
    "        return backoff\n",
    "ct = combined_tagged(train_data = train_data,\n",
    "                     taggers = [UnigramTagger,BigramTagger,TrigramTagger],\n",
    "                    backoff = rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917610610901345\n"
     ]
    }
   ],
   "source": [
    "print(ct.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(ct.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在在测试数据上获得了91%的准确率。对于最终的标签器，我们将使用有监督的分类算法来训练他，ClassifierBasedPOSTager类使我们能够使用classifier_builder参数中的有监督机器学习算法来训练标签器。该类继承自ClassifierBasedTaggr，并拥有构成训练核心部分的feature_detector()函数，该函数用于从训练数据中生成各种特征，实际上在实例化ClassifierBasedPOSTagger类对象时，你也可以构建自己的特征检测器函数，将其传递给feature_detector参数，在这里我们使用的分类器是NaiveBayesClassifier，它使用贝叶斯定理构建概率分类器，假设特征之间是独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码展示了如何基于分类构建POS标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import  NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "nbt = ClassifierBasedPOSTagger(train = train_data,\n",
    "                              classifier_builder = NaiveBayesClassifier.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "print(nbt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306806079969019\n"
     ]
    }
   ],
   "source": [
    "print(nbt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于分类器的POS标签器很强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将句子分解为最小的组成部分，然后将他们组合成更高级的短语。在浅层分析中，主要的关注焦点是识别这些短语或语块，而不是挖掘各个块内句法和语句关系的深层细节，正如我们在基于深度分析获得的分析树中看到的，浅层分析的主要目的是获得语义上有意义的短语，并观察他们之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.浅层分析器推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用pattern包创建一个浅层分析器，用以从句子中提取有意义的语块，以下代码段展示了如何在我们的例句上执行浅层分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "from pattern.en import parsetree\n",
    "tree = parsetree(sentence)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP -> [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]\n",
      "VP -> [('is', 'VBZ')]\n",
      "ADJP -> [('quick', 'JJ')]\n",
      "NP -> [('he', 'PRP')]\n",
      "VP -> [('is', 'VBZ'), ('jumping', 'VBG')]\n",
      "PP -> [('over', 'IN')]\n",
      "NP -> [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type,'->',[(word.string,word.type) for word in chunk.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parsetree,Chunk\n",
    "from nltk.tree import Tree\n",
    "def creat_sentence_tree(sentence,lemmatize = False):\n",
    "    sentence_tree = parsetree(sentence,\n",
    "                             relations = True,\n",
    "                             lemmata = lemmatize)\n",
    "    return sentence_tree[0]\n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "def process_sentence_tree(sentence_tree):\n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "        (item.type,\n",
    "         [\n",
    "             (w.string,w.type)\n",
    "             for w in item.words\n",
    "         ]\n",
    "        )\n",
    "        if type(item) == Chunk\n",
    "        else(\n",
    "        '-',\n",
    "        [\n",
    "            (item.string,item.type)\n",
    "        ])\n",
    "        for item in tree_constituents\n",
    "    ]\n",
    "    return processed_tree\n",
    "def print_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ]\n",
    "            )\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S',processed_tree)\n",
    "    print (tree)\n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ])\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S',processed_tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "t = creat_sentence_tree(sentence)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NP', [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]),\n",
       " ('VP', [('is', 'VBZ')]),\n",
       " ('ADJP', [('quick', 'JJ')]),\n",
       " ('-', [('and', 'CC')]),\n",
       " ('NP', [('he', 'PRP')]),\n",
       " ('VP', [('is', 'VBZ'), ('jumping', 'VBG')]),\n",
       " ('PP', [('over', 'IN')]),\n",
       " ('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = process_sentence_tree(t)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "print_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的输出显示了从例句中创建、表示和可视化浅层分析树的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建自己的浅层分析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用正则表达式、基于标签的学习器等技术构建自己的浅层分析器。与之前的POS标签类似，如果需要的话，我们会使用一些训练数据来训练分析器，然后使用测试数据和例句对分析器进行评估。在nltk中，可以使用treebank语料库，它带有语块标注。首先加载语料库，并使用以下代码段准备训练数据集和测试数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print (train_data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通过使用RegexpParser类，我们可以利用正则表达式创建浅层分析器，以说明名词短语的分块和加缝隙过程，如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "simple_sentence = 'the quick fox jumped over the lazy dog'\n",
    "from nltk.chunk import RegexpParser\n",
    "from pattern.en import  tag\n",
    "tagged_simple_sent = tag(simple_sentence)\n",
    "print(tagged_simple_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT quick/JJ fox/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "chunk_grammer =\"\"\"NP:{<DT>?<JJ>*<NN.*>}\"\"\"\n",
    "rc = RegexpParser(chunk_grammer)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chink_grammer = \"\"\"NP:{<.*>+}#chunk everything as NP}<VBD|IN>+{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP\n",
      "    the/DT\n",
      "    quick/JJ\n",
      "    fox/NN\n",
      "    jumped/VBD\n",
      "    over/IN\n",
      "    the/DT\n",
      "    lazy/JJ\n",
      "    dog/NN))\n"
     ]
    }
   ],
   "source": [
    "rc = RegexpParser(chink_grammer)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在试验性NP（名词短语）浅层分析器上使用分块和加缝隙方法得到了相同的结果，请记住，语块是包含在组块（语块）集合中的标识序列，缝隙则是被排除在语块外的标识或表示序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们要训练一个更为通用的基于正则表达式的浅层分析器，并在我们的测试treebank数据上检测其性能。在程序内部，需要执行几个步骤来完成此分析器。首选，需要将nltk中用于表示被解析语句的Tree结构转换为ChunkString对象，然后使用定义好的分块和加缝隙规则创建一个RegexpParse对象。最后使用ChunkRule和ChinkRule类及其对象创建完整的、带有必要语块的浅层分析树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentence = tag(sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}\n",
    "ADJP: {<JJ>}\n",
    "ADVP:{<RB.*>}\n",
    "PP: {<IN>}\n",
    "VP: {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sentence)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  54.5%%\n",
      "    Precision:     25.0%%\n",
      "    Recall:        52.5%%\n",
      "    F-Measure:     33.9%%\n"
     ]
    }
   ],
   "source": [
    "print(rc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将使用分块好并且标记好的treebank训练数据，构建一个浅层分析器。我们会用到两个分块函数，一个是tree2conlltags函数，它可以为每个词元获取三组数据--单词、标签和块标签；另一个函数是conlltags2tree函数，它可以从上述三元组数据中生成分析树。稍后，我们将使用这些函数来训练分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk.util import tree2conlltags,conlltags2tree\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT', 'B-NP'),\n",
       " ('Lorillard', 'NNP', 'I-NP'),\n",
       " ('spokewoman', 'NN', 'I-NP'),\n",
       " ('said', 'VBD', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('``', '``', 'O'),\n",
       " ('This', 'DT', 'B-NP'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('an', 'DT', 'B-NP'),\n",
       " ('old', 'JJ', 'I-NP'),\n",
       " ('story', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtc = tree2conlltags(train_sent)\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(wtc)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经知道了这些函数是如何工作的，接下来，我们定义一个函数conlltag_chunks()从分块标注好的句子中提取POS和块标签，我们再次使用ocmbined_taggers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t,c)for (w,t,c) in sent] for sent in tagged_sents]\n",
    "def combined_tagger(train_data,taggers,backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data,backoff = backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们定义一个NgramTagChunker类，将标记好的句子作为训练输入，获取他们的WTC三元组，即单词、POS标签和块标签三元组，并使用UnigramTagger作为backoff标签器训练一个BigramTagger。我们还定义一个parse()函数来对新的句子执行浅层分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger,BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    def __init__(self,train_sentences,tagger_classes = [UnigramTagger,BigramTagger]):\n",
    "        train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "        self.chunk_tagger = combined_tagger(train_sent_tags,tagger_classes)\n",
    "    def parse(self,tagged_sentence):\n",
    "        if not tagged_sentence:\n",
    "            return None\n",
    "        pos_tags = [tag for word,tag in tagged_sentence]\n",
    "        chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "        chunk_tags = [chunk_tag for (pos_tag,chunk_tag) in chunk_pos_tags]\n",
    "        wpc_tags = [(word,pos_tag,chunk_tag)for ((word,pos_tag),chunk_tag)in zip(tagged_sentence,chunk_tags)]\n",
    "        return conlltags2tree(wpc_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述类中，构造函数__init__()使用基于语句WTC三元组的n元分词标签训练浅层分析器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  99.6%%\n",
      "    Precision:     98.4%%\n",
      "    Recall:       100.0%%\n",
      "    F-Measure:     99.2%%\n"
     ]
    }
   ],
   "source": [
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  is/VBZ\n",
      "  (NP quick/JJ)\n",
      "  and/CC\n",
      "  (NP he/PRP)\n",
      "  is/VBZ\n",
      "  jumping/VBG\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "tree = ntc.parse(tagged_sentence)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们在conll2000语料库上对我们的分析器进行训练和评估。conll2000语料库是一个更大的语料库，它包含了“华尔街日报”摘录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.4%%\n",
      "    Precision:     80.8%%\n",
      "    Recall:        86.0%%\n",
      "    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "tc = NGramTagChunker(train_wsj_data)\n",
    "print (tc.evaluate(test_wsj_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于依存关系的分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
