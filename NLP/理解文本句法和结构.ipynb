{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS标签器推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在这里讨论一些标记句子的推荐方法，第一种方法是使用推荐的ｐｏｓ＿ｔａｇ（）函数，它基于Penn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('is', 'VERB'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('is', 'VERB'), ('jumping', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens,tagset = 'universal')\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以使用pattern模块通过以下代码段获取句子的pos标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sent = tag(sentence)\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立自己的POS标签器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将讨论一些构建自己的POS标签器的技术，并利用nltk提供的一些类来实现他们。为了评估我们的标签器的性能，我们会使用nltk中treebank语料库的一些测试数据。我们还将使用一些训练数据来训练标签器。首先通过读取已标记的treebank语料库，我们可以获取训练和评估标签器的必要数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用测试数据来评估标签器，并使用例句的标识作为输入来验证标签器的工作效果，我们在nltk中使用的所有标签器均来自nltk.tag包。每个标签器都是基类Tagger1类的子类，并且每个标签器都执行一个tag()函数，它将一个句子的标识列表作为输入，返回带有POS标签的相同单词列表作为输出。除了标记外，还有一个evaluate()函数用于评估标签器的性能。它通过标记每个输入测试语句，然后将输出结果与句子的实际标签进行对比来完成评估。我们将使用该函数来测试我们的标签器在test_data上的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们看看从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "print(dt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NN'), ('jumping', 'NN'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的输出可以看出，在树库(treebank)测试数据集中我们已经获得了14%的单词正确标记率，并不是很理想，现在我们将使用正则表达式和RegexpTagger来尝试构建一个性能更好的标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21425113757382128\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "patterns = [\n",
    "    (r'.*ing$','VBG'),      #动名词\n",
    "    (r'.*ed$','VBD'),       #一般过去式\n",
    "    (r'.*es$','VBZ'),       #3rd singular present\n",
    "    (r'.*ould$','MD'),      #modals\n",
    "    (r'.*\\'s$','NN$'),      #possessive nouns\n",
    "    (r'.*s$','NNS'),        #plural nouns\n",
    "    (r'^->[0-9]+(.[0-9]+)?$','CD'),#cardinal numbers\n",
    "    (r'.*','NN')            #nouns (default)...\n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "print (rt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'NNS'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('is', 'NNS'), ('jumping', 'VBG'), ('over', 'NN'), ('the', 'NN'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在将训练一些n元分词标签器，n元分词是来自文本序列或者语言序列的n个连续项。这些项可以由单词、音素、字母、字符或音节组成。Shingles是只包含单词的n元分词，我们将使用大小为1，2，3的n元分词，他们分别成为一元分词，二元分词和三元分词。UnigramTagger、BigramTagger和TrigramTagger继承自基类NGramTagger，NGramTagger类则继承自ContextTagger类，该类又继承自SequentialBackoffTagger类。我们将使用train_data作为训练数据，根据与举报标识及其POS标签来训练n元分词标签器，然后我们将在test_data上评估训练后的标签器，并查看例句的标记结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607803272340013\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)\n",
    "\n",
    "print(ut.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(ut.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13466937748087907\n"
     ]
    }
   ],
   "source": [
    "print(bt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(bt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08064672281924679\n"
     ]
    }
   ],
   "source": [
    "print(tt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('is', None), ('quick', None), ('and', None), ('he', None), ('is', None), ('jumping', None), ('over', None), ('the', None), ('lazy', None), ('dog', None)]\n"
     ]
    }
   ],
   "source": [
    "print(tt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的数据清楚的表明，我们仅使用UnigramTagger标签器就可以在测试集上获得86%的准确率，这个结果与我们前一个标签器相比要好得多，二元分词与三元分词模型的准确性远不及一元分词模型，因为在训练数据中观察到的二元词组与三元词组不一定会在测试数据中以相同的方式出现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在通过创建一个包含标签列表的组合标签器以及使用backoff标签器，我们将尝试组合运用所有的标签器。本质上我们将创建一个标签器链，对于每一个标签器，如果他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_tagged(train_data,taggers,backoff = None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data,backoff = backoff)\n",
    "        return backoff\n",
    "ct = combined_tagged(train_data = train_data,\n",
    "                     taggers = [UnigramTagger,BigramTagger,TrigramTagger],\n",
    "                    backoff = rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917610610901345\n"
     ]
    }
   ],
   "source": [
    "print(ct.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'NN'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(ct.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在在测试数据上获得了91%的准确率。对于最终的标签器，我们将使用有监督的分类算法来训练他，ClassifierBasedPOSTager类使我们能够使用classifier_builder参数中的有监督机器学习算法来训练标签器。该类继承自ClassifierBasedTaggr，并拥有构成训练核心部分的feature_detector()函数，该函数用于从训练数据中生成各种特征，实际上在实例化ClassifierBasedPOSTagger类对象时，你也可以构建自己的特征检测器函数，将其传递给feature_detector参数，在这里我们使用的分类器是NaiveBayesClassifier，它使用贝叶斯定理构建概率分类器，假设特征之间是独立的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码展示了如何基于分类构建POS标签器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import  NaiveBayesClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "nbt = ClassifierBasedPOSTagger(train = train_data,\n",
    "                              classifier_builder = NaiveBayesClassifier.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "print(nbt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306806079969019\n"
     ]
    }
   ],
   "source": [
    "print(nbt.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于分类器的POS标签器很强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅层分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将句子分解为最小的组成部分，然后将他们组合成更高级的短语。在浅层分析中，主要的关注焦点是识别这些短语或语块，而不是挖掘各个块内句法和语句关系的深层细节，正如我们在基于深度分析获得的分析树中看到的，浅层分析的主要目的是获得语义上有意义的短语，并观察他们之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.浅层分析器推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用pattern包创建一个浅层分析器，用以从句子中提取有意义的语块，以下代码段展示了如何在我们的例句上执行浅层分析："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "from pattern.en import parsetree\n",
    "tree = parsetree(sentence)\n",
    "\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk('The brown fox/NP'), Chunk('is/VP'), Chunk('quick/ADJP'), Chunk('he/NP'), Chunk('is jumping/VP'), Chunk('over/PP'), Chunk('the lazy dog/NP')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    print(sentence_tree.chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP -> [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]\n",
      "VP -> [('is', 'VBZ')]\n",
      "ADJP -> [('quick', 'JJ')]\n",
      "NP -> [('he', 'PRP')]\n",
      "VP -> [('is', 'VBZ'), ('jumping', 'VBG')]\n",
      "PP -> [('over', 'IN')]\n",
      "NP -> [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence_tree in tree:\n",
    "    for chunk in sentence_tree.chunks:\n",
    "        print(chunk.type,'->',[(word.string,word.type) for word in chunk.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import parsetree,Chunk\n",
    "from nltk.tree import Tree\n",
    "def creat_sentence_tree(sentence,lemmatize = False):\n",
    "    sentence_tree = parsetree(sentence,\n",
    "                             relations = True,\n",
    "                             lemmata = lemmatize)\n",
    "    return sentence_tree[0]\n",
    "def get_sentence_tree_constituents(sentence_tree):\n",
    "    return sentence_tree.constituents()\n",
    "def process_sentence_tree(sentence_tree):\n",
    "    tree_constituents = get_sentence_tree_constituents(sentence_tree)\n",
    "    processed_tree = [\n",
    "        (item.type,\n",
    "         [\n",
    "             (w.string,w.type)\n",
    "             for w in item.words\n",
    "         ]\n",
    "        )\n",
    "        if type(item) == Chunk\n",
    "        else(\n",
    "        '-',\n",
    "        [\n",
    "            (item.string,item.type)\n",
    "        ])\n",
    "        for item in tree_constituents\n",
    "    ]\n",
    "    return processed_tree\n",
    "def print_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ]\n",
    "            )\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S',processed_tree)\n",
    "    print (tree)\n",
    "def visualize_sentence_tree(sentence_tree):\n",
    "    processed_tree = process_sentence_tree(sentence_tree)\n",
    "    processed_tree = [\n",
    "        Tree(item[0],\n",
    "            [\n",
    "                Tree(x[1],[x[0]])\n",
    "                for x in item[1]\n",
    "            ])\n",
    "        for item in processed_tree\n",
    "    ]\n",
    "    tree = Tree('S',processed_tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox is quick and he is jumping over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "t = creat_sentence_tree(sentence)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NP', [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]),\n",
       " ('VP', [('is', 'VBZ')]),\n",
       " ('ADJP', [('quick', 'JJ')]),\n",
       " ('-', [('and', 'CC')]),\n",
       " ('NP', [('he', 'PRP')]),\n",
       " ('VP', [('is', 'VBZ'), ('jumping', 'VBG')]),\n",
       " ('PP', [('over', 'IN')]),\n",
       " ('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = process_sentence_tree(t)\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (DT The) (JJ brown) (NN fox))\n",
      "  (VP (VBZ is))\n",
      "  (ADJP (JJ quick))\n",
      "  (- (CC and))\n",
      "  (NP (PRP he))\n",
      "  (VP (VBZ is) (VBG jumping))\n",
      "  (PP (IN over))\n",
      "  (NP (DT the) (JJ lazy) (NN dog)))\n"
     ]
    }
   ],
   "source": [
    "print_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualize_sentence_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的输出显示了从例句中创建、表示和可视化浅层分析树的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建自己的浅层分析器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用正则表达式、基于标签的学习器等技术构建自己的浅层分析器。与之前的POS标签类似，如果需要的话，我们会使用一些训练数据来训练分析器，然后使用测试数据和例句对分析器进行评估。在nltk中，可以使用treebank语料库，它带有语块标注。首先加载语料库，并使用以下代码段准备训练数据集和测试数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (train_data[7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
