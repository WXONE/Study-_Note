{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类也称文本归类，这里使用文本分类这个词有两个原因，第一个原因是我们要分类文档，文本分类和文本归类具有相同的本质，第一个原因是我们要分类文档，文本分类和文本归类具有相同的本质。第二个原因是我们将用分类或有监督机器学习方法来分类或归类文档。文本分类具有很多方法。我们将会集中精力解释用于分类的有监督方法。分类过程不只局限于文本，还广泛用于其他领域，包括科学、健康、天气预测和技术等\n",
    "假设有一个预定义的类集合，文本或文档分类是将文档指定到一个或多个分类或类型的过程。这里的文档就是文本文档，每个文档分类到正确的类别中。真实的文档D可能拥有很多内在的属性，这使得D成为高维空间的一个实体。使用这个空间的一个子集，其实包含一组有限的描述或特征的集合，表示为d，可以使用文本分类系统T成功的将原始文档D划分到正确的类型C。这可以表示为T：D->C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种基于文档内容类型的分类：\n",
    "*基于内容的分类\n",
    "*基于请求的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为实现自动文本分类，可以充分利用一些机器学习的技术和概念。这里主要有两类与解决该问题相关的技术\n",
    "* 有监督学习\n",
    "* 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "有一个文档集合TS，集合中文档带有相应的类别或分类标签，这是一个文档和标签对的集合，TS = {(d1,c1),(d2,c2),....,(dn,cn)}，其中d1,d2,....,dn是文本列表，c1,c2,...,cn是这些文本对应的类型。假设我们已经有了训练数据集，我们可以定义一个有监督学习算法F，当算法在训练数据TS集上训练之后，我们得到训练好的分类器γ，可以表示为F(TS) = γ。因此，有监督学习算法F使用输入集对TS，得到训练的分类器γ--这就是我们的模型，上述过程就称之为训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个需要记住的关键点就是有监督文本分类也需要一些手工标注的训练数据，即使我们谈的是自动文本分类，我们也需要一些手工的工作以启动我们的自动处理。当然，这一点的收益也是多方面的，我们可以使用较少的努力和人力监督来不停的进行新文档的预测与分类。\n",
    "基于预测类型的数量和预测的本质，有多种文本分类。这些类型基于数据集、与数据集相关的类型或类别数量或类别数量、数据点上可以预测的类型数量。\n",
    "* 二元分类是当离散类型或类别的数量是2时，任何预测可以是二者之一。\n",
    "* 多元分类是指当全部类型数量超过2时，这是二元分类问题的一个扩展。\n",
    "* 多标签分类问题指的是对于任何数据，每个预测结果可以产生多个结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类的蓝图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类系统典型工作流程的主要步骤：\n",
    "* 准备训练和测试数据\n",
    "* 文本规范化处理\n",
    "* 特征抽取\n",
    "* 模型训练\n",
    "* 模型预测与评估\n",
    "* 模型部署\n",
    "为建立文本分类器，需要按照顺序执行这些步骤。\n",
    "在训练过程中，每个文档都有对应的分类或类型，这些分类或类型是提前手工标注和组织的。这些训练文档在文本规范化模块中处理和规范化，输出整齐和标准化的文档。接着将他们送入特征提取模块，这一模块使用不同的特征提取技术从文档中提取有意义的特征。\n",
    "训练模型过程需要将文档的特征向量和每个文档对应的标签送入，使得算法可以学习每个分类或类型对用的不同模式，可以冲哟个这些学习到的知识预测未来新文档的分类。一般情况下，使用一个可选的验证数据集来评估分类算法的性能，以确保算法使用训练过程中的数据获得较好的推广能力。训练过程结束后，这些特征和机器学习算法的组合产生了分类模型。通常情况下，会使用不同的参数对这个模型进行调优以获得一个性能更好的模型，这一过程成为超参数优化\n",
    "测试数据集文档经过同样的规范化处理和特称提取过程后，这些文档的特征被送到训练好的分类模型，这个模型根据前期训练好的模式预测每个文档可能的类标签，如果有手工标注的这些文档的真实类标签，你可以通过使用不同度量标准(比如准确率)比较真实标签和预测标签，评估这个模型的性能。这将反映模型对于新文档的预测性能。\n",
    "一旦获得了一个稳定的、可工作的模型，最后一步通常是部署这个模型，这包括存储这个模型和相关依赖的文件，将模型部署为一个服务或者可执行程序，他批量预测新文档的类型，或以web服务的形式满足用户的请求。这里有很多不同的机器学习模型部署方法，这通常取决于你后续如何访问这些模型。\n",
    "我们将讨论前面蓝图中的一些主要模块，并且实现这些模型，以便我们可以将这些模块集成在一起，建立一个真实的文本分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本规范化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将定义一个规范化模块以处理文本文档规范化。我们将坚持简化与直接原则，以便于很容易一步一步参照这里的实现。我们将在模型中实现和使用下面的规范化技术：\n",
    "* 扩展缩写词\n",
    "* 通过词形还原实现文本处理规范化\n",
    "* 去除特殊字符和符号\n",
    "* 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizerrdNetLemmatizer\n",
    "stopword_list = nltk.corpus.stopword_list.words('endlish')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们载入了英文停用词、出自CONTRACTION_MAP的缩写映射和WordNetLemmatizer的一个实例来实现词形还原。现在我们定义一个函数实现文本的切分，他将用在其他的规范化函数中。下面的函数实现词语切分，并去除分割后符号中的多余空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义扩展缩写词的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text,contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                     flags = re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match,text)\n",
    "    expanded_text = re.sub(\"'\",\"\",expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来实现一个使用词形还原函数把单词变换为词基或词根形式的函数以对文本进行规范化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def pos_tag_text(text):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(),penn_to_wn_tags(pos_tag))\n",
    "                        for word,pos_tag in \n",
    "                        tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatize_tokens = [wnl.lemmatize(word,pos_tag)if pos_tag\n",
    "                       else word \n",
    "                       for word,pos_tag in pos_tagged_text]\n",
    "    lemmatize_text = ' '.join(lemmatize_tokens)\n",
    "    return lemmatize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码片段描述了两个词形还原函数。主函数是lemmatize_text,该函数接受文本数据，基于每个此行标签还原词形，接着给用户返回词形还原处理后的文本，为实现这个功能，需要标注每个文本符号的词性标签。使用pattern函数库中的tag函数对每个符号标注词性标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数帮我们实现特殊符号和字符的去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token)for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然已经定义了全部函数，就可以通过将所有函数一个一个的连接在一起的方式建立文本处理流水线。下面的函数实现上述功能，输入文本文档语料，进行规范化处理，返回规范化处理后的文本文档语料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,tokenize = False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text,CONTRACTION_MAP)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "        return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我们完成了文本规范化处理模块所需的全部函数的讨论和实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在代码实现中，将使用gensim库。基本思想是提供一些文档语料作为输入，会得到词向量特征作为输出，在模型内部建立基于输入文档的词汇表学习单词的向量表示方法，一旦学习完成就建立了一个可用于从文档中提取单词向量的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练语料建立模型时，我们将使用下面的参数：\n",
    "* size：该参数用于设定词向量的维度，可以是几十到几千，可以尝试不同的维度，以获得最好的效果。\n",
    "* window：该参数用于设定语境或窗口尺寸，指定了训练时对算法来说可算做上下文的单词窗口长度。\n",
    "* min_count：该参数指定单词表中单词在语料中出现的最小次数。这个参数有助于去除一些文档中出现次数较少的不重要的单词。\n",
    "* sample：该参数用于对单词出现的频率进行下采样，其理想值在0.01到0.0001之间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立模型后，将基于一些加权策略来定义和实现两种词向量与文档结合的技术。接下来将实现下面两个技术：\n",
    "* 平均词向量\n",
    "* TF-IDF加权词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进一步实现之前，使用训练语料建立word2vec模型，开始特征提取的过程。下面的代码显示了如何实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "new_doc = ['loving this blue sky today']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in CORPUS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_new_doc = [nltk.word_tokenize(sentence) for sentence in new_doc]\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS,size = 10,window = 10,min_count = 2,sample = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了自己的模型，可以开始实现特征提取技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 平均词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的模型为单词表中的每个单词创建一个向量表示，我们可以输入下面的代码来查看他们"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6986217e-02 -2.4031328e-02 -2.3780285e-05 -2.6104983e-02\n",
      "  5.7248911e-03 -1.1811212e-03 -2.3749880e-02  2.2004517e-02\n",
      " -4.4517949e-02  3.9885491e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print (model['sky'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03806227  0.01623118 -0.01578265  0.0374135   0.02925476  0.01199543\n",
      "  0.0089135   0.02041917  0.04870435 -0.04785519]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于前面指定的参数尺寸，每个单词向量的长度为10.但是当我们处理语句和文档时，必须执行一些合并和聚合操作，以确保无论文本长度、单词数量等情况如何，最后特征的维度必须时相同的。在这个技术中，我们使用平均加权词向量，对于每个文档我们将提取文档中所有单词，获得单词表中每个单词的词向量。我们将全部词向量加在一起，除以单词表中匹配单词的总数，最后得到文档的平均词向量表示结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def average_word_vertors(words,model,vocabulary,num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype = \"float64\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector,model[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector,nwords)\n",
    "    return feature_vector\n",
    "def averaged_word_vectorizer(corpus,model,num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vertors(tokenized_sentence,model,vocabulary,num_features)\n",
    "               for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面代码显示了我们这个函数在示例语料库上的执行情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.    -0.013  0.012  0.019  0.011 -0.01  -0.016  0.024 -0.003  0.014]\n",
      " [-0.006 -0.007  0.016  0.02   0.016 -0.022 -0.007  0.021 -0.018  0.013]\n",
      " [-0.005 -0.011  0.013  0.022  0.017 -0.017 -0.004  0.021 -0.008  0.007]\n",
      " [ 0.038  0.016 -0.016  0.037  0.029  0.012  0.009  0.02   0.049 -0.048]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "avg_wordvec_features = averaged_word_vectorizer(corpus = TOKENIZED_CORPUS,model = model ,num_features=10)\n",
    "print(np.round(avg_wordvec_features,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经获得了语料库中每个文档的维度尺寸同意的平均词向量，这些词向量将在后面送入机器学习算法进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF加权平均词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的向量生成器基于模型单词表中的单词，简单的对任何文档中相关的词向量进行求和，通过除以匹配的单词的数量计算一个简单的平均值。下面我们使用单词的TF-IDF评分对每个匹配的词向量进行加权，对他们进行求和并除以文档中匹配的单词数量，我们将得到每个文档的一个TF-IDF加权平均词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features):\n",
    "    word_tfidfs = [tfidf_vector[0,tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word)\n",
    "                  else 0 for word in words]\n",
    "    word_tfidfs_map = {word:tfidf_val for word , tfidf_val in zip(words,word_tfidfs)}\n",
    "    feature_vector = np.zeros((num_features,),dtype = \"float64\")\n",
    "    vocabulary = set(model.mv.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidfs_map[word]*word_vector\n",
    "            wts = wts + word_tfidfs_map[word]\n",
    "            feature_vector = np.add(feature_vector,weighted_word_vector)\n",
    "        if wts:\n",
    "            feature_vector = np.divide(feature_vector,wts)\n",
    "        return feature_vector\n",
    "    def tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features):\n",
    "        docs_tfidfs = [(doc,doc_tfidf) for doc,docs_tfidf in zip(corpus,tfidf_vectors)]\n",
    "        features = [tfidf_weighted_averaged_word_vectorizer(tokenized_sentence,tfidf,tfidf_model,num_features)\n",
    "                   for tokenized_sentence,tfidf in docs_tfidfs]\n",
    "        return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidf_wtd_avg_word_vectors()函数帮助我们获得每个文档的TD-IDF加权平均词向量，我们也创建一个函数tfidf_weighted_averaged_word_vectorizer()实现语料库中多个文档TF-IDF加权平均词向量的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类算法是有监督的机器学习算法，基于其对过去的观测情况，对数据点进行分类，归类或加标签。作为一个有监督的机器学习算法，每个分类算法都需要训练数据。训练数据由一组训练值组成，其中每个观测值是一对输入数据点(通常是特征向量，如我们前面看到的)和输入观测值对应的输出结果。分类算法整体有三个过程：\n",
    "* 训练是有监督学习算法分析和尝试推理训练数据的模式,让算法可以识别出产生具体输出模式的过程。这些输出一般是类标签、类变化、响应变量。训练之前，我们通常执行特征提取过程从原始数据中提取出有意义的特征。这些特征集被送入我们所选择的算法，这个算法尽力从这些数据和对应的输出中识别和学习模式。算法的结果是一个推断函数，成为模型或分类模型。期望该模型从训练集的学习模式中得到足够的推广能力，使之能够预测未来新数据点的类别或结果。\n",
    "* 评估包括测试模型的预测性能，检验它在训练数据集上训练和学习的效果如何。为此，我们通常使用验证数据集，通过预测该数据集，并对比预测结果与真实类标签的差异来测试模型的性能，真实类标签也称为真实值。通常我们还会使用交叉验证，其中把数据划分为2分，其中一份用于训练，剩下的用于验证被训练的模型。注意，我们还会根据验证结果对模型进行调整，以获得最佳的配置，从而产生最高的精度和最小的误差。我们也会使用保留数据集或测试数据集来评估我们的模型，但我们不会使用这个数据集调整模型，因为这将导致模型有偏差或者把数据过拟合到非常具体的特征。保留数据集或测试数据集是新的、有代表性的样本点，真实的数据样本可以看到模型产生预测的情况，以及模型在这些新的数据样本上的效果如何。稍后，我们将研究用来评估和度量模型性能的各种常用的度量标准。\n",
    "* 调优，也称超参数调优或优化，我们致力于优化模型以最大限度地发挥其预测能力并减少错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们主要使用如下算法：\n",
    "* 多项式朴素贝叶斯\n",
    "* 支持向量机"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
