{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids = 'carroll-alice.txt')\n",
    "sample_text = 'We will discuss briefly about the basic syntax,structure and design philosophies.There is a defined hierarchical syntax for Python code which you should remember when writing code !Python is a really powerful programming language!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n"
     ]
    }
   ],
   "source": [
    "print (len(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.sent_tokenize 函数是nltk推荐的默认的句子切分函数。它内部使用了一个PunktSentenceTokenizer类的实例，然而，它不仅仅是一个普通的对象或实例——它已经在几种语言模型上完成了预训练(牛批)，并且在除英语外的许多主流语言上取得了良好的运行效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码展示了该函数在示例文本中的基本用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text = alice)\n",
    "sample_sentences = default_st(text = sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 1\n"
     ]
    }
   ],
   "source": [
    "print ('Total sentences in sample_text:',len(sample_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text sentences:-\n",
      "['We will discuss briefly about the basic syntax,structure and design '\n",
      " 'philosophies.There is a defined hierarchical syntax for Python code which '\n",
      " 'you should remember when writing code !Python is a really powerful '\n",
      " 'programming language!']\n"
     ]
    }
   ],
   "source": [
    "print('Sample text sentences:-')\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences in alice: 1625\n"
     ]
    }
   ],
   "source": [
    "print('\\nTotal sentences in alice:',len(alice_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences in alice\"-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "print('First 5 sentences in alice\"-')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词语切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主流接口：\n",
    "word_tokenize\n",
    "\n",
    "TreebankWordTokenizer\n",
    "\n",
    "RegexpTokenizer\n",
    "\n",
    "从RegexpTokenizer继承的切分器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用例句“The brown fox wasn't that quick and he couldn't win the race \"作为各种切分器的输入，nltk.word_tokenize 函数是nltk默认并且推荐的词语切分器。该切分器实际上是TreebankWordTokenizer类的一个实例或者对象，并且是该核心类的一个封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用正则表达式和RegeTokenizer类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOKEN_PATTERN = r'\\w+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "regex_wt = nltk.RegexpTokenizer(pattern = TOKEN_PATTERN,gaps = False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern = GAP_PATTERN,gaps = True)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print (word_indices)\n",
    "print([sentence[start:end]for start ,end in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本规范化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本规范化定义为这样的一个过程，它包含一系列步骤，一次是转换，清洗以及将文本数据标准化成可供NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常成为文本清洗或转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race \",\n",
    "         \"Hey that's a great deal ! I just bought a phone for 199\",\n",
    "         \"@@ You'll (learn) a **lot** in the book .Python is an amazing language!@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用nltk的clean_html函数清洗来自HTML的不必要的标记，甚至是BeautifulSoup库来解析HTML数据，你还可以使用自定义的逻辑，包括正则表达式，xpath和lxml来解析xml数据。从JSON获取数据较为容易，因为它具有明确的键值注释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里介绍一个通用的切分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence)for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的功能是接受文本数据，再从中提取句子，最后将每个句子划分成标识，这些标识可以是单词、特殊字符或标点符号。以下代码说明了该函数的功能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.Python',\n",
      "   'is',\n",
      "   'an',\n",
      "   'amazing',\n",
      "   'language',\n",
      "   '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text)for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除特殊字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标点和特殊字符往往没什么意义，我们将在切分前后删除这两类特殊字符，以下显示了在切分之后删除特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token)for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x0000014F6F9D67F0>, <filter object at 0x0000014F6F9D69B0>, <filter object at 0x0000014F6F9D6C18>]\n"
     ]
    }
   ],
   "source": [
    "filtered_list_1 = [filter(None,[remove_characters_after_tokenization(tokens)for tokens in sentence_tokens])for sentence_tokens in token_list]\n",
    "print (filtered_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在文本切分之前删除特殊字符（推荐）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence,keep_apostrophes = False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'#add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9]'#only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN,r'',sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thebrownfoxwasntthatquickandhecouldntwintherace', 'HeythatsagreatdealIjustboughtaphonefor199', 'YoulllearnalotinthebookPythonisanamazinglanguage']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence)for sentence in corpus]\n",
    "print (filtered_list_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal ! I just bought a phone for 199\", \" You'll learn a lot in the book .Python is an amazing language!\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence,keep_apostrophes = True)for sentence in corpus]\n",
    "print(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展缩写词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缩写词是词或者音节的缩短形式，他们既在书面形式中存在，也在口语中存在，比如“is not”缩写为“isn't”，缩写词中撇号用来表示缩写，而一些元音和其他字母则被删除了，通常，在正式书写时会避免使用缩写词，但在非正式情况下，他们被广泛使用。\n",
    "\n",
    "缩写词为NLP和文本分析制造了一个难题，首先因为在该单词中有一个特殊的撇号字符，此外，我们有两个甚至更多的单词由缩写词表示。\n",
    "\n",
    "可以使用映射关系来扩展缩写词，我们创建了一个缩写词及其扩展形式的词汇表，你可以在Python库中的contractions.py 中访问他们"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 大小写转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数lower()和upper() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "停用词是指没有或只有极小意义的词语，通常在处理过程中将他们从文本中删除，以保留具有最大意义及语境的词语，如果基于单个标识聚合语料库，然后检查词语频率，就会发现停用词的出现频率是最高的，类似\"a\",\"the\",\"me\",\"and so on\"这样的单词或词组就是停用词，每个领域都有可能有一系列独用的停用词。以下代码展示了一种过滤和删除英语停用词的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个函数中，nltk中有一个英文的停用词表。我们使用tokeniez_text 函数来分割在上一节中获得的expanded_corpus然后使用前面的函数删除停用词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expanded_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-1b61e00d1375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexpanded_corpus_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexpanded_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfiltered_list_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;32mfor\u001b[0m \u001b[0msentence_tokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexpanded_corpus_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_list_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'expanded_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text)for text in expanded_corpus]\n",
    "filtered_list_3 = [[remove_stopwords(tokens)for tokens in sentence_tokens]for sentence_tokens in expanded_corpus_tokens]\n",
    "print(filtered_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看到了89页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
