# CS224 Skip-Gram神经网络语言模型

## 导学：

今天我将把我的项目经验结合CS224n的前两讲的内容与大家交流。

Word2Vec目标函数的梯度，简单介绍Word2Vec，我们想要获取一个词向量，将他作为一个词义模型，这其实是一个有争议的想法，我先来介绍一下来龙去脉，然后深入研究。

如果你在词典里查meaning的意思，韦氏词典对meaning的表述是：用单词、短语等表示的想法，人们想要通过单词、符号表达的想法，从单词的角度讲，这非常接近于语言学上最常理解的meaning的含义，在标准的语言学中，单词指代了一种符号，他代表世界上的某些具体事物，比如说鼠标，我的鼠标，和世界上所有的鼠标，就是“鼠标”一词的指代物，目前这种对meaning的解释并未应用到计算机技术中。

到目前为止，最常见的办法是用分类资源来处理词义，在英语中最著名的分类资源是WordNet。他展示了很多分类。

```python
from nltk.corpus import wordnet as wn
panda = wn.synset('panda.n.01')
hyper = lambda s : s.hypernyms()
list[panda.colsure(hyper)]
```

用nltk导入wordnet包

但是使用WordNet会使人们遗漏掉很多细节，人们在使用词汇的时候会更加灵活，比如说，`i am a deep-learning ninja`,这在WordNet语料库里就没有。人们往同义词集里面加什么词是一个非常主观的选择，你如何区分一个词有哪些含义，那些词是不一样的，是非常模糊的选择。最终很难对词汇的相似性给出准确的含义。这就是一个问题，这就是离散化表征普遍存在的问题，现在需要注意的问题是，几乎所有的NLP研究，除了现代深度学习，以及八十年代所做的一点NLP神经网络外，几乎所有的NLP都使用了原子符号来表示单词，这个原子符号就是我们接下来要说的词向量。

## 1. 词向量

​	最早的词向量是很冗长的，它使用词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将其对应的位置设置为1.比如我们有下面的5个词组成的词汇表，词"Queen"的序号为2， 那么它的词向量就是(0,1,0,0,0)(0,1,0,0,0)。同样的道理，词"Woman"的词向量就是(0,0,0,1,0)(0,0,0,1,0)。这种词向量的编码方式我们一般叫做1-of-N representation或者one hot representation.（独热编码），独热编码表示词向量非常简单，但是问题也很明显，最大的问题就是我们的词汇非常大，wiki中文语料库的容量高达1.6GB，一个汉字所占16Byte，可想而知这个语料库里要有多少汉字。所以在这时候我们就需要一个百万维的向量来表示，但是这对内存来说就是一场灾难。这样的向量其实除了一个位置是1，其余全是0，表达的效率很低。任意两个词的独热编码都是正交的，` 两个向量 a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：a·b=a1b1+a2b2+……+anbn。 ` 这就体现出一个问题：他不能给出任何词汇之间的内在关系概念。当然我们可以使用独热编码去完成一些任务，但是这里我不建议大家这样做。

比如说king 、 queen 、 man 、woman 、child

(画图)

![img](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\1042406-20170713145606275-2100371803.png)

​	

我们可以选择一种直接的方法， 一个单词所表示的含义，是你可以直接阅读的，在这些表示中，你就可以看出相似性，我们要做的就是构建这些向量，然后做一些类似求解点积的操作。

我们应该怎么做呢，我们就需要使用这个十分简单，极其玄妙，又应用广泛的NLP技术，称作分布相似性（distributional  similarity）。分布相似性是指，你可以得到大量表示某个词汇含义的值，只需通过观察其出现的上下文，并对这些上下文做一些处理来得到。这是一种关于词汇语义的理论。

`government debt problems turning into banking crises as has happended in saying that Europe needs unified banking regulation to replace the hodgepodge`

在这个语料库中，如果我想知道 `banking `这个单词的意思，我需要做的就是找到数个这个单词出现的句子，然后观察他每一次出现的语境，我会看到他的上下文：......等等等等，然后我开始统计所有出现过的内容，通过某种方式用这些上下文中的词来表示banking的含义 。

​	具有相似上下文的词，应该有相似的语义，这个假说被称为distributional hypothesis，分布式表示就是一种能够刻画语义之间的相似度并且维度较低的稠密向量表示，例如：

>高兴=[0.2,1.6,−0.6,0.7,0.3]T
>
>开心=[0.3,1.4,−0.5,0.9,0.2]T

这样，便可通过计算向量的余弦相似度，来反映出词的语义相似度。

余弦相似度计算：

![img](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\20131111175301140)

![img](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\20131111175458906)

如果向量a和b不是二维而是n维，上述余弦的计算法仍然正确。假定a和b是两个n维向量，a是  ，b是  ，则a与b的夹角 的余弦等于：

![img](https://img-blog.csdn.net/20131111175544093)

余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，夹角等于0，即两个向量相等，这就叫"余弦相似性"。



Distributed representation（分布式表示）可以解决One hot representation的问题，它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这个较短的词向量维度是多大呢？这个一般需要我们在训练时自己来指定。

分布式表示的含义与之前讲过的词汇含义的指称是完全不一样的，鼠标的意思是指我们手里的一个个具体的鼠标，这就有异于分布式表示。分布式表示又与独热编码不同，独热编码是一种存储在某处的本地表示，对于某个词，我们在所有任务中，他的独热编码都是固定的

当我们拿到一个语料库时，我们首先要对其进行清洗、分词（明天再讲），之后我们要做的就是给每一个单词构造一个向量，我们会选择一个密集型向量，让她可以预测目标单词所在文本的其他词汇。

所有其他词汇也是由一个个单词构成的，我们看一下相似性测量的方法，比如两个向量间的点积，我们将对他进行一些修改，以便于它可以预测，我们有很多算法来完成这些工作，这样以来这些词汇就可以预测上下文的其他词汇，也可以通过他上下文的词汇来预测他 。

​	　比如下图我们将词汇表里的词用"Royalty","Masculinity", "Femininity"和"Age"4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)。当然在实际情况中，我们并不能对词向量的每个维度做一个很好的解释。

![img](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\1042406-20170713150625759-1047275185.png)

　　有了用Distributed Representation表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：

![img](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\1042406-20170713151608181-1336632086.png)

​						King→−Man→+Woman→=Queen→

 　　　　可见我们只要得到了词汇表里所有词对应的词向量，那么我们就可以做很多有趣的事情了。不过，怎么训练得到合适的词向量呢？一个很常见的方法是使用神经网络语言模型。



ok这是第一部分，现在我们进入第二部分，什么是Word2Vec。

# Word2Vec

这是我们一种通用的方法来学习神经词嵌入问题(neural word embedding)。

我们定义一个模型，通过中心词汇预测他上下文的词汇，就像上面讲过的分布式学习。我们有一些概率的方法来实现。接下来，我们用损失函数来判断预测的准确性。理想状态下，我们可以准确预测中心词汇周围的词

![1557753074186](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\%5CUsers%5CWXONE%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1557753074186.png)

wt是中心词汇，w-t是除了他以外的所有其他的上下文。



这里的-t表示环绕在t中心词周围的其他单词，如果我们能准确预测这些词，则 J = 0，我们就没有损失，但是正常情况下我们做不到这一点，我们得到了损失函数将会在语料库的各个地方执行这种预测操作，目的是调整词汇表示，从而使损失最小化。

但是，你实际上除了设置一个这样的预测目标外，其他的什么也没做，或者说做不了，只是让每个单词的向量都能预测其周围的词汇。你只是顶下这样简明的目标，其他的什么也没做，这就是深度学习的魅力。所以我们要深究一下细节，看看这些是如何实现的。

如果我们只想得到好的单词表示，我们甚至不需要构建一个可以预测的概率语言模型，我们只需要知道一种学习单词表示的方法。 这就是Word2Vec模型。

Word2Vec模型的核心是构建一个很简单的、可扩展的、快速的训练模型，让我们可以处理数十亿单词的文本，并生成非常棒的向量表示。

Word2Vec模型尝试去做的最基本的事就是利用语言的意义理论，来预测每个单词和他的上下文词汇，word2vec是一个软件，里面有两个生成词向量的算法，里面还有两套效率中等的训练方法，今天我们只讲一个成为skip-gram的方法

![1557754170789](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\%5CUsers%5CWXONE%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1557754170789.png)

# Skip-Gram用于神经网络语言模型

![1557754236210](D:\Study-_Note\NLP\斯坦福自然语言处理\图片\%5CUsers%5CWXONE%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1557754236210.png)

这是一张Skip-Gram模型的图，这个模型的概念是，在每一个估算步都取一个词作为中心词汇，我们仍然用上面的那个语料：

`government debt problems turning into banking crises as has happended in saying that Europe needs unified banking regulation to replace the hodgepodge`

这里我们选择中心词banking，尝试用他去预测一定范围内的上下文词汇，所以这个模型将定义一个概率分布，即给定一个中心词汇，计算某个单词在他上下文中出现的概率