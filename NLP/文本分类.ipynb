{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类也称文本归类，这里使用文本分类这个词有两个原因，第一个原因是我们要分类文档，文本分类和文本归类具有相同的本质，第一个原因是我们要分类文档，文本分类和文本归类具有相同的本质。第二个原因是我们将用分类或有监督机器学习方法来分类或归类文档。文本分类具有很多方法。我们将会集中精力解释用于分类的有监督方法。分类过程不只局限于文本，还广泛用于其他领域，包括科学、健康、天气预测和技术等\n",
    "假设有一个预定义的类集合，文本或文档分类是将文档指定到一个或多个分类或类型的过程。这里的文档就是文本文档，每个文档分类到正确的类别中。真实的文档D可能拥有很多内在的属性，这使得D成为高维空间的一个实体。使用这个空间的一个子集，其实包含一组有限的描述或特征的集合，表示为d，可以使用文本分类系统T成功的将原始文档D划分到正确的类型C。这可以表示为T：D->C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种基于文档内容类型的分类：\n",
    "*基于内容的分类\n",
    "*基于请求的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为实现自动文本分类，可以充分利用一些机器学习的技术和概念。这里主要有两类与解决该问题相关的技术\n",
    "* 有监督学习\n",
    "* 无监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "有一个文档集合TS，集合中文档带有相应的类别或分类标签，这是一个文档和标签对的集合，TS = {(d1,c1),(d2,c2),....,(dn,cn)}，其中d1,d2,....,dn是文本列表，c1,c2,...,cn是这些文本对应的类型。假设我们已经有了训练数据集，我们可以定义一个有监督学习算法F，当算法在训练数据TS集上训练之后，我们得到训练好的分类器γ，可以表示为F(TS) = γ。因此，有监督学习算法F使用输入集对TS，得到训练的分类器γ--这就是我们的模型，上述过程就称之为训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个需要记住的关键点就是有监督文本分类也需要一些手工标注的训练数据，即使我们谈的是自动文本分类，我们也需要一些手工的工作以启动我们的自动处理。当然，这一点的收益也是多方面的，我们可以使用较少的努力和人力监督来不停的进行新文档的预测与分类。\n",
    "基于预测类型的数量和预测的本质，有多种文本分类。这些类型基于数据集、与数据集相关的类型或类别数量或类别数量、数据点上可以预测的类型数量。\n",
    "* 二元分类是当离散类型或类别的数量是2时，任何预测可以是二者之一。\n",
    "* 多元分类是指当全部类型数量超过2时，这是二元分类问题的一个扩展。\n",
    "* 多标签分类问题指的是对于任何数据，每个预测结果可以产生多个结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类的蓝图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类系统典型工作流程的主要步骤：\n",
    "* 准备训练和测试数据\n",
    "* 文本规范化处理\n",
    "* 特征抽取\n",
    "* 模型训练\n",
    "* 模型预测与评估\n",
    "* 模型部署\n",
    "为建立文本分类器，需要按照顺序执行这些步骤。\n",
    "在训练过程中，每个文档都有对应的分类或类型，这些分类或类型是提前手工标注和组织的。这些训练文档在文本规范化模块中处理和规范化，输出整齐和标准化的文档。接着将他们送入特征提取模块，这一模块使用不同的特征提取技术从文档中提取有意义的特征。\n",
    "训练模型过程需要将文档的特征向量和每个文档对应的标签送入，使得算法可以学习每个分类或类型对用的不同模式，可以冲哟个这些学习到的知识预测未来新文档的分类。一般情况下，使用一个可选的验证数据集来评估分类算法的性能，以确保算法使用训练过程中的数据获得较好的推广能力。训练过程结束后，这些特征和机器学习算法的组合产生了分类模型。通常情况下，会使用不同的参数对这个模型进行调优以获得一个性能更好的模型，这一过程成为超参数优化\n",
    "测试数据集文档经过同样的规范化处理和特称提取过程后，这些文档的特征被送到训练好的分类模型，这个模型根据前期训练好的模式预测每个文档可能的类标签，如果有手工标注的这些文档的真实类标签，你可以通过使用不同度量标准(比如准确率)比较真实标签和预测标签，评估这个模型的性能。这将反映模型对于新文档的预测性能。\n",
    "一旦获得了一个稳定的、可工作的模型，最后一步通常是部署这个模型，这包括存储这个模型和相关依赖的文件，将模型部署为一个服务或者可执行程序，他批量预测新文档的类型，或以web服务的形式满足用户的请求。这里有很多不同的机器学习模型部署方法，这通常取决于你后续如何访问这些模型。\n",
    "我们将讨论前面蓝图中的一些主要模块，并且实现这些模型，以便我们可以将这些模块集成在一起，建立一个真实的文本分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本规范化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将定义一个规范化模块以处理文本文档规范化。我们将坚持简化与直接原则，以便于很容易一步一步参照这里的实现。我们将在模型中实现和使用下面的规范化技术：\n",
    "* 扩展缩写词\n",
    "* 通过词形还原实现文本处理规范化\n",
    "* 去除特殊字符和符号\n",
    "* 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizerrdNetLemmatizer\n",
    "stopword_list = nltk.corpus.stopword_list.words('endlish')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们载入了英文停用词、出自CONTRACTION_MAP的缩写映射和WordNetLemmatizer的一个实例来实现词形还原。现在我们定义一个函数实现文本的切分，他将用在其他的规范化函数中。下面的函数实现词语切分，并去除分割后符号中的多余空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义扩展缩写词的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_contractions(text,contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                     flags = re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match,text)\n",
    "    expanded_text = re.sub(\"'\",\"\",expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来实现一个使用词形还原函数把单词变换为词基或词根形式的函数以对文本进行规范化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def pos_tag_text(text):\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(),penn_to_wn_tags(pos_tag))\n",
    "                        for word,pos_tag in \n",
    "                        tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatize_tokens = [wnl.lemmatize(word,pos_tag)if pos_tag\n",
    "                       else word \n",
    "                       for word,pos_tag in pos_tagged_text]\n",
    "    lemmatize_text = ' '.join(lemmatize_tokens)\n",
    "    return lemmatize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码片段描述了两个词形还原函数。主函数是lemmatize_text,该函数接受文本数据，基于每个此行标签还原词形，接着给用户返回词形还原处理后的文本，为实现这个功能，需要标注每个文本符号的词性标签。使用pattern函数库中的tag函数对每个符号标注词性标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数帮我们实现特殊符号和字符的去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None,[pattern.sub('',token)for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然已经定义了全部函数，就可以通过将所有函数一个一个的连接在一起的方式建立文本处理流水线。下面的函数实现上述功能，输入文本文档语料，进行规范化处理，返回规范化处理后的文本文档语料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,tokenize = False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text,CONTRACTION_MAP)\n",
    "        text = lemmatize_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            normalized_corpus.append(text)\n",
    "            \n",
    "        return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我们完成了文本规范化处理模块所需的全部函数的讨论和实现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
